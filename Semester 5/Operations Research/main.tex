\documentclass{report}

\input{../template/preamble}
\input{../template/macros}
\input{../template/letterfonts}

\title{\Huge{Operations Research}\\Semester 5}
\author{Ahmad Abu Zainab}
\date{}

\usepackage[customcolors]{hf-tikz}

\begin{document}

\maketitle
\newpage% or \cleardoublepage
% \pdfbookmark[<level>]{<title>}{<dest>}
\pdfbookmark[section]{\contentsname}{toc}
\tableofcontents
\pagebreak

\chapter{Linear Programming}

A linear programming problem is a problem in the form
\[
	\max Z = \sum_{i=0 }^{n } (c_i)x_i
	.\]
Subject to
\[
	\arraycolsep=1.4pt
	\begin{array}{rrrrrrr}
		a_{11}x_1 & + & \cdots & + & a_{1n}x_n & \leq               & b_1 \\
		a_{21}x_1 & + & \cdots & + & a_{2n}x_n & \leq               & b_2 \\
		          &   &        &   &           & \vdotswithin{\leq} &     \\
		a_{n1}x_1 & + & \cdots & + & a_{nn}x_n & \leq               & b_1
	\end{array}
	.\]
Where $x_i$ are the decision variables

In matrix form

\[
	\max Z = \vb{c}^T \vb{x}
	.\]

Subject to

\[
	\vb{A}\vb{x} \leq \vb{b}
	.\]

\[
	\vb{x} \geq \vb{0}
	.\]
\section{Simplex Method}

\subsection{Augmented Form}
First we need to convert the problem to the standard form

\[
	\max Z = \vb{c}^T \vb{x}
	.\]

Subject to

\[
	\vb{A}\vb{x} = \vb{b}
	.\]

\[
	\vb{x} \geq \vb{0}
	.\]

Then we transform the problem to the augmented form by adding slack variables. We add $m$ slack variables to the problem, where $m$ is the number of constraints.

\[
	\max Z = \vb{c}^T \vb{x}
	.\]

Subject to

\[
	\vb{A}\vb{x} + \vb{I}\vb{s} = \vb{b}
	.\]

\[
	\vb{x}, \vb{s}, \vb{b} \geq \vb{0}
	.\]

Where $\vb{I}$ is the identity matrix and $\vb{s}$ is the vector of slack variables.

\subsection{Basic Feasible Solution}

A basic solution is a solution where $n$ of the variables are set to zero and the rest are set to the $m$ values of the corresponding entries in $\vb{b}$.

A basic \emph{feasible} solution is a basic solution where all the variables are non-negative, and it corresponds to a vertex of the feasible region. Two adjacent vertices share all but one basic variable.

Variables set to zero are called non-basic variables, and the rest are called basic variables. The choice of basic variables is called the basis.

A basic feasible solution is called degenerate if one of the basic variables is zero.

\subsection{Initialising the Simplex Method}

We set up the initial tableau by adding the slack variables and the objective function to the augmented form, as follows

\[
	\begin{array}{c|c|c|c|c|c|c|c|c|c}
		\text{BV} & x_1    & x_2    & \cdots & x_n    & s_1    & s_2    & \cdots & s_m    & \text{RHS} \\
		\hline
		Z         & -c_1   & -c_2   & \cdots & -c_n   & 0      & 0      & \cdots & 0      & 0          \\
		s_1       & a_{11} & a_{12} & \cdots & a_{1n} & 1      & 0      & \cdots & 0      & b_1        \\
		s_2       & a_{21} & a_{22} & \cdots & a_{2n} & 0      & 1      & \cdots & 0      & b_2        \\
		\vdots    & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots     \\
		s_m       & a_{m1} & a_{m2} & \cdots & a_{mn} & 0      & 0      & \cdots & 1      & b_m        \\
		\hline
	\end{array}
	.\]

Let's take an example

\[
	\max Z = 3x_1 + 5x_2
	.\]

Subject to

\[
	\arraycolsep=1.4pt
	\begin{array}{rrrrrrr}
		x_1  &   &      & \leq & 4  \\
		     &   & 2x_2 & \leq & 12 \\
		3x_1 & + & 2x_2 & \leq & 18
	\end{array}
	.\]

\[
	x_1, x_2 \geq 0
	.\]

First we convert the problem to the augmented form by adding slack variables

\begin{center}
	\fbox{
		\begin{minipage}{0.35\textwidth}
			$\max Z = 3x_1 + 5x_2$\\\\
			Subject to\\
			$
				\arraycolsep=1.4pt
				\begin{array}{rrrrrrrrr}
					x_1  &   &      & + & s_1 &     &     & = & 4  \\
					     &   & 2x_2 & + &     & s_2 &     & = & 12 \\
					3x_1 & + & 2x_2 & + &     &     & s_3 & = & 18
				\end{array}
			$
		\end{minipage}
	}
\end{center}

Then we set up the initial tableau

\[
	\begin{array}{c|c|c|c|c|c|c}
		\text{BV} & x_1 & x_2 & s_1 & s_2 & s_3 & \text{RHS} \\
		\hline
		Z         & -3  & -5  & 0   & 0   & 0   & 0          \\
		s_1       & 1   & 0   & 1   & 0   & 0   & 4          \\
		s_2       & 0   & 2   & 0   & 1   & 0   & 12         \\
		s_3       & 3   & 2   & 0   & 0   & 1   & 18         \\
		\hline
	\end{array}
	.\]


First, we identify the entering variable. The entering variable is the variable with the most negative coefficient in the objective function.
In this case, the entering variable is $x_2$.

\[
	\begin{array}{c|c|c|c|c|c|c}
		\text{BV} & x_1 & \tikzmarkin{col}(0.15,-0.3)(-0.05,0.4)x_2 & s_1 & s_2 & s_3 & \text{RHS} \\
		\hline
		Z         & -3  & -5                                        & 0   & 0   & 0   & 0          \\
		s_1       & 1   & 0                                         & 1   & 0   & 0   & 4          \\
		s_2       & 0   & 2                                         & 0   & 1   & 0   & 12         \\
		s_3       & 3   & 2 \tikzmarkend{col}                       & 0   & 0   & 1   & 18         \\
		\hline
	\end{array}
	.\]


Then, we identify the leaving variable. The leaving variable is the variable with the smallest ratio of the RHS to the coefficient of the entering variable in the objective function.
In this case, the leaving variable is $s_2$.

\[
	\begin{array}{c|c|c|c|c|c|c}
		\text{BV}                                  & x_1 & x_2 & s_1 & s_2 & s_3 & \text{RHS}           \\
		\hline
		Z                                          & -3  & -5  & 0   & 0   & 0   & 0                    \\
		s_1                                        & 1   & 0   & 1   & 0   & 0   & 4                    \\
		\tikzmarkin{row}(0.05,-0.1)(-0.05,0.35)s_2 & 0   & 2   & 0   & 1   & 0   & 12 \tikzmarkend{row} \\
		s_3                                        & 3   & 2   & 0   & 0   & 1   & 18                   \\
		\hline
	\end{array}
	.\]


Then we perform the pivot operation on the leaving variable and the entering variable. The pivot operation is performed by dividing the row of the leaving variable by the coefficient of the entering variable in that row, and then subtracting the resulting row from the other rows, multiplied by the coefficient of the entering variable in that row.

After the pivot operation, the tableau becomes

\[
	\begin{array}{c|c|c|c|c|c|c}
		\text{BV} & x_1 & x_2 & s_1 & s_2 & s_3 & \text{RHS} \\
		\hline
		Z         & -3  & 0   & 0   & 2.5 & 0   & 30         \\
		s_1       & 1   & 0   & 1   & 0   & 0   & 4          \\
		x_2       & 0   & 1   & 0   & 0.5 & 0   & 6          \\
		s_3       & 3   & 0   & 0   & -1  & 1   & 6          \\
		\hline
	\end{array}
	.\]

Finally, we repeat the process until the objective function has no negative coefficients.
\nt{
	The \emph{optimality test} is as follows
	\begin{itemize}
		\item If the objective function has no negative coefficients, then the current solution is optimal.
		\item If the objective function has negative coefficients, then the current solution is not optimal, and we repeat the process.
	\end{itemize}
}

In our case, after iterating another time ($x_1$ entering and $s_3$ leaving), we get

\[
	\begin{array}{c|c|c|c|c|c|c}
		\text{BV} & x_1 & x_2 & s_1 & s_2  & s_3  & \text{RHS} \\
		\hline
		Z         & 0   & 0   & 0   & 1.5  & 1    & 36         \\
		s_1       & 0   & 0   & 1   & 1/3  & -1/3 & 2          \\
		x_2       & 0   & 1   & 0   & 0.5  & 0    & 6          \\
		x_1       & 1   & 0   & 0   & -1/3 & 1/3  & 2          \\
		\hline
	\end{array}
	.\]

Since the objective function has no negative coefficients, the current solution is optimal. The optimal solution is $x_1 = 2, x_2 = 6, s_1 = 2, s_2 = 0, s_3 = 0$ and $Z = 36$.

\subsection{Unbounded Solution}

In a given tableau, if the entering variable has all zero entries in its column, then the solution is unbounded. Which means that the objective function can be increased indefinitely.

\subsection{Alternative Optimal Solutions}

Alternative optimal solutions occur when one of the non-basic variables has a zero coefficient in the objective function in the final tableau. In this case, the solution is degenerate.

\subsection{Non-Standard Form}

\begin{align*}
	0.4x_1 - 0.3x_2 \geq -10 & \overset{\times -1}{\implies} -0.4x_1 + 0.3x_2 \leq 10  \\
	\min Z = 0.4x_1 + 0.3x_2 & \overset{\times -1}{\implies} \max Z = -0.4x_1 - 0.3x_2 \\
\end{align*}

\subsection{Artificial Variables}

Consider the following linear system

\[
	\vb{A}\vb{x} = \vb{b}
	.\]
and
\[
	\vb{x} \geq \vb{0}
	.\]

We can use the simplex method to solve this system by adding artificial variables to the system, as follows

\[
	\vb{A}\vb{x} + \vb{I}\vb{a} = \vb{b}
	.\]

\[
	\vb{x}, \vb{a} \geq \vb{0}
	.\]

Where $\vb{I}$ is the identity matrix and $\vb{a}$ is the vector of artificial variables.

Take the following example

\[
	\arraycolsep=1.4pt
	\begin{array}{rrrrrrr}
		x_1 & + & 2x_2 & + & x_3 & = & 4 \\
		x_1 & + & x_2  &   &     & = & 3
	\end{array}
	.\]

The augmented form is

\[
	\arraycolsep=1.4pt
	\begin{array}{rrrrrrrrrr}
		x_1 & + & 2x_2 & + & x_3 & + & a_1 &     & = & 4 \\
		x_1 & + & x_2  &   &     & + &     & a_2 & = & 3
	\end{array}
	.\]

And we can solve it using the simplex method.

\subsubsection{Two-Phase Method}

In the case where the basic feasible solution isn't very obvious, we can use the two-phase method to find the basic feasible solution by adding artificial variables to the system, as follows

\[
	\vb{A}\vb{x} + \vb{I}\vb{s} + \vb{I}\vb{a} = \vb{b}
	.\]

\[
	\vb{x}, \vb{s}, \vb{a} \geq \vb{0}
	.\]

Where $\vb{I}$ is the identity matrix and $\vb{s}$ is the vector of slack variables and $\vb{a}$ is the vector of artificial variables.

In the first phase we solve the problem starting from the BFS where all the artificial variables are basic variables. If the problem is feasible, we will be able to find a BFS where all artificial variables are 0. This automatically gives us a BFS for the original problem.

For example, consider the following problem

\[
	\min Z = 2x_1 + 3x_2
	.\]
Subject to
\[
	\arraycolsep=1.4pt
	\begin{array}{rrrrr}
		x_1  & + & 2x_2 & = & 4 \\
		2x_1 & - & x_2  & = & 3 \\
	\end{array}
	.\]

We can convert it to the following augmented form

\[
	\arraycolsep=1.4pt
	\begin{array}{rrrrrrrrrr}
		x_1  & + & 2x_2 &  &   & +   & a_1 & = & 4 \\
		2x_1 & - & x_2  &  & + & a_2 &     & = & 3
	\end{array}
	.\]

We define a new objective function

\[
	\max W = -a_1 - a_2
	.\]

The initial tableau is set up as follows

\[
	\begin{array}{c|c|c|c|c|c|c|}
		\text{BV} & x_1 & x_2 & a_1 & a_2 & \text{RHS} \\
		\hline
		W         & -3  & -1  & 0   & 0   & 7          \\
		Z         & -2  & -3  & 0   & 0   & 0          \\
		a_1       & 1   & 2   & 1   & 0   & 4          \\
		a_2       & 2   & -1  & 0   & 1   & 3          \\
		\hline
	\end{array}
	.\]

The resulting solution is a BFS for the original problem. In the second phase we solve the original problem starting from the BFS we found in the first phase.

\subsubsection{Big-$M$ Method}

The Big-$M$ method is a variation of the two-phase method where we add a large number $M$ to the objective function for each artificial variable. This ensures that the artificial variables will be set to zero in the optimal solution.

The objective function looks like this

\[
	\max Z = \vb{c}^T \vb{x} - M \vb{a}
	.\]

The initial simplex tableau is set up as follows

\[
	\begin{array}{c|c|c|c|c|c|c|c|c|c}
		\text{BV} & x_1    & x_2     & \cdots & x_n    & s_1    & s_2    & \cdots & s_m    & \text{RHS} \\
		\hline
		Z         & -c_1   & -c_2    & \cdots & -c_n   & M      & M      & \cdots & M      & 0          \\
		s_1       & a_{11} & a_{12}  & \cdots & a_{1n} & 1      & 0      & \cdots & 0      & b_1        \\
		s_2       & a_{21} & a_{22}  & \cdots & a_{2n} & 0      & 1      & \cdots & 0      & b_2        \\
		\vdots    & \vdots & \vdots  & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots     \\
		s_m       & a_{m1} & a _{m2} & \cdots & a_{mn} & 0      & 0      & \cdots & 1      & b_m        \\
		\hline
	\end{array}
	.\]

\subsection{Infeasible LPs}

An LP is infeasible if in the final tablea there are still artificial variables that are basic variables.

\subsection{Constraints with $\geq$}

If we have a contraint with $\geq$ and a negative RHS we can multiply the constraint by $-1$ and proceed as usual.
However, if we have a constraint with $\geq$ and a positive RHS we turn the constraint into an equality by adding a slack variable as seen below

\[
	x_1-2x_2+x_3\geq20\quad\implies\quad x_1-2x_2+x_3-s_1=20
	.\]

Where $s_1>0$. \\

But for initial BFS we cannot simplex $s_1$ in to the base so we need to add an artificial variable $a_1$ to the contraint as if it were an equality contraint

\[
	x_1-2x_2+x_3-s_1+a_1=20
	.\]

Where $a_1>0$.

\subsection{Variables Unconstrained in Sign}

If we have a variable that is unconstrained in sign we can split it into two variables ${x_1}^+$ and ${x_1}^-$ where $x_1={x_1}^+-{x_1}^-$.

\[
	{x_1}^+ = \max(0,x_1) \quad {x_1}^- = \max(0,-x_1)
	.\]

\subsection{Maximize the Minimum}

Some real life problems consist of maximizing the lowest of many economic functions.

\[
	\max Z = \min(3x_1+2x_2, x_1-2x_3)
	.\]

We can solve this problem by adding a new variable $u$ and adding the following constraints

\[
	\arraycolsep=1.4pt
	\begin{array}{rrrrrrr}
		3x_1 & + & 2x_2 & - & u & \geq & 0 \\
		x_1  & - & 2x_3 & - & u & \geq & 0
	\end{array}
	.\]

where $u$ is unconstrained in sign.

\chapter{Network Optimization Models}

\section{Max Flow Problem}

Consider a directed graph $G=(V,E)$ with a source node $s$ and a sink node $t$. Each edge $(i,j)\in E$ has a capacity $u_{ij}$. The max flow problem is to find the maximum flow from $s$ to $t$.

\paragraph{LP Model}

\[
	\max Z = \sum_{i\in\delta^+(s)} x_{si}
	.\]

or

\[
	\max Z = \sum_{i\in\delta^-(t)} x_{it}
	.\]

where $\delta^+(s)$ is the set of nodes that are flowing out of to $s$ and $\delta^-(t)$ is the set of nodes that are flowing in to $t$.\\

Subject to

\[
	-u_{ij} \leq x_{ij} \leq u_{ij}
	.\]

and

\[
	\sum_{i\in\delta^+(j)} x_{ij} = \sum_{i\in\delta^-(j)} x_{ji} \quad \forall j\in V\setminus\{s,t\}
	.\]

\section{Min Cost Flow Problem}

Consider a directed graph $G=(V,E)$. Each edge $(i,j)\in E$ has a capacity $u_{ij}$, a cost $c_{ij}$, and a supply $b_i$. The min cost flow problem is to find the minimum cost flow within this network.

\paragraph{LP Model}

\[
	\min Z = \sum_{(i,j)\in E} c_{ij}x_{ij}
	.\]

Subject to

\[
	\sum_{i\in\delta^+(j)} x_{ji} - \sum_{i\in\delta^-(j)} x_{ij} = b_j \quad \forall j\in V
	.\]

and

\[
	x_{ij} \leq u_{ij} \quad \forall (i,j)\in E
	.\]

\nt{
	The supply $b_i$ is positive if the node is a source, negative if the node is a sink, and zero if the node is a transshipment node.
}

\nt{
	If the capacity $u_{ij}$ is not specified, we can assume it is $\infty$. (The edge is unconstrained in capacity)
}

\subsection{Special Cases}

\subsubsection{Transportation Problem}

Sources have $b_i>0$, sinks have $b_i<0$, and transshipment nodes have $b_i=0$. All nodes have unlimited capacity, and edges all have a cost $c_{ij}$.

\subsubsection{Shortest Path Problem}

Sources have $b_i=1$, sinks have $b_i=-1$, and transshipment nodes have $b_i=0$. All nodes have unlimited capacity, and edges all have a cost $c_{ij}$.

\subsubsection{Sink Tree Problem}

Sources have $b_i=\text{number of nodes in the network}$, and at every other node $b_i=-1$. All nodes have unlimited capacity, and edges all have a cost $c_{ij}$.

\subsubsection{Max Flow Problem}

At the source we set $b=M$, where $M$ is a very large number, and we set $b=-M$ for target nodes. We then create a dummy node $d$ and connect it to the source and target nodes with unlimited capacity and cost 1. The simplex algorithm will route flow through the non-dummy links since they have 0 cost.

\thm{Integrality Theorem}{
	If all $b_i$ and $u_{ij}$ are integers, then there exists an optimal solution where all $x_{ij}$ are integers.
}

\section{Shortest Path Problem}

Given a directed graph $G=(V,E)$ with a source node $s$ and a sink node $t$. Each edge $(i,j)\in E$ has a cost $c_{ij}$. The shortest path problem is to find the shortest path (minimum cost) from $s$ to $t$.

\paragraph{LP Model}

\[
	\min Z = \sum_{(i,j)\in E} c_{ij}x_{ij}
	.\]

Subject to

\[
	\sum_{i\in\delta^+(j)} x_{ji} - \sum_{i\in\delta^-(j)} x_{ij} = \begin{cases}
		1  & \text{if } j=s   \\
		-1 & \text{if } j=t   \\
		0  & \text{otherwise}
	\end{cases}
	.\]

where $x_{ij}$ is a binary value (0 or 1).

\subsection{Dijkstra's Algorithm}

Given the same setup as the shortest path problem, we can solve it using Dijkstra's algorithm. The algorithm is as follows

\begin{enumerate}
	\ii Set $d(s)=0$ and $d(i)=\infty$ for all other nodes.
	\ii Set $S=\emptyset$ (the set of visited nodes).
	\ii Set $i=s$.
	\ii Find the node $j$ with the smallest $d(j)$ such that $j\notin S$.
	\ii Add $j$ to $S$.
	\ii For all $k\notin S$, set $d(k)=\min(d(k),d(j)+c_{jk})$.
	\ii If $j\neq t$, set $i=j$ and go to step 4.
	\ii Repeat step 4 until $S=V$.
\end{enumerate}

\begin{algorithm}
	\SetAlgoLined
	\KwResult{Shortest path from $s$ to $t$}
	\SetKwFunction{FMain}{Dijkstra}
	\SetKwProg{Fn}{Function}{:}{}
	\Fn{\FMain{$G,s,t$}}{
	$S\gets\emptyset$\;
	$d(s)\gets 0$\;
	\While{$t\notin S$}{
	$j\gets\argmin_{j\notin S} d(j)$\;
	$S\gets S\cup\{j\}$\;
	\For{$k\notin S$}{
	$d(k)\gets\min(d(k),d(j)+c_{jk})$\tcc*[r]{$c_{ij}$ being the cost of the edge}
	}
	}
	\Return{$d(t)$}\;
	}
	\caption{Dijkstra's Algorithm}
\end{algorithm}

\newpage

\section{Minimum Spanning Tree Problem}

Given a connected undirected graph $G=(V,E)$ with a cost $c_{ij}$ for each edge $(i,j)\in E$. The minimum spanning tree problem is to find the minimum cost tree that connects all nodes.\\

We can solve this using an algorithm called Prim's algorithm. The algorithm is as follows

\begin{enumerate}
	\ii Set $S=\{s\}$ (the set of visited nodes).
	\ii Set $i=s$.
	\ii Find the edge $(i,j)$ with the smallest $c_{ij}$ such that $j\notin S$.
	\ii Add $j$ to $S$.
	\ii If $S=V$, stop.
	\ii Set $i=j$ and go to step 3.
\end{enumerate}

\begin{algorithm}
	\SetAlgoLined
	\KwResult{Minimum spanning tree}
	\SetKwFunction{FMain}{Prim}
	\SetKwProg{Fn}{Function}{:}{}
	\Fn{\FMain{$G$}}{
		$S\leftarrow\{s\}$\;
		\While{$S\neq V$}{
			$(i,j)\leftarrow\argmin_{j\notin S} c_{ij}$\;
			$S\leftarrow S\cup\{j\}$\;
		}
		\Return{$S$}\;
	}
	\caption{Prim's Algorithm}
\end{algorithm}

\chapter{Transportation and Assignment Problems}

\section{Transportation Problem}

The transportation problem is an LP problem where we have a set of $m$ supply nodes and a set of $n$ demand nodes. Each supply node has a supply (capacity) $s_i$ and each demand node has a demand $-d_j$. The cost of transporting one unit $x_{ij} = 1$ from supply node $i$ to demand node $j$ is $c_{ij}$ . The transportation problem is to find the minimum cost of transporting the supply to the demand. The total cost of shipping is

\[
	\sum_{i=1}^{m}\sum_{j=1}^{n} c_{ij}x_{ij}
	.\]

In a balanced transportation problem, the total supply is equal to the total demand.

\[
	\sum_{i=1}^{m} s_i = \sum_{j=1}^{n} d_j
	.\]

\subsection{LP Model}

\[
	\min Z = \sum_{i=1}^{m}\sum_{j=1}^{n} c_{ij}x_{ij}
	.\]

Subject to

\begin{align*}
	\sum_{j=1}^{n} x_{ij} & = s_i \quad \forall i    \\
	\sum_{i=1}^{m} x_{ij} & = d_j \quad \forall j    \\
	x_{ij}                & \geq 0 \quad \forall i,j
\end{align*}

The LP has

\begin{itemize}
	\ii $mn$ variables
	\ii $m+n$ equality constraints
	\ii One redundant constraint
	\ii The effective number of constraints is $m+n-1$
	\ii The rank of the constraint matrix is $m+n-1$
\end{itemize}

Due to the shape of the problem the simplex method is not efficient for solving the transportation problem. Instead we use the transportation algorithm

\begin{enumerate}
	\ii For the non-redundant constraints, we select the next basic variable according to a rule (northwest corner rule or the Vogel rule).
	\ii Set that basic varaible to the largest possible value. This is either the remaining supply or the remaining demand, whichever is smaller.
	\ii Update the supply and demand and remove the constraint.
\end{enumerate}

If both the supply and demand are zero, then we eliminate the row and assume that the demand is 0. We will have a degenerate basic variable in the final solution.

\thm{Northwest rule}{
	We start at the northwest corner of the cost matrix and allocate as much as possible to the basic variable regardless of the cost.
}

\thm{Vogel's rule}{
	We calculate the difference between the two smallest costs for each row and each column. We then select the row or column with the largest difference and allocate as much as possible to the basic variable.
}

\subsection{Special Cases}

\begin{description}
	\ii[Prohibited Routes] If a route is prohibited, we can set the cost to $M$ ($\infty$).
	\ii[Excess Supply] If there is excess supply, we can add a dummy demand node with a demand of the excess supply and a cost of 0. The quantity shipped to the dummy node is "throw away".
	\ii[Excess Demand(shortage)] If there is excess demand, we can add a dummy supply node with a supply of the excess demand and a cost of 0. The quantity shipped from the dummy node is "how much the destination will be missing out".
	\begin{itemize}
		\ii Distribute the available production with minimal shipping cost, ignoring fairness.
		\ii Guarantee some "minimum" to some (or all) destinations. Split destination $D_j$ in 2:
		\begin{itemize}
			\ii $D_j^1$ with demand $d_j^1=$ minimum demand and cost $c_{ij}^1=\infty$.
			\ii $D_j^2$ with the remaining demand ($d_j^2=d_j-d_j^1$) and cost $c_{ij}^2=0$.
		\end{itemize}
		\ii \textbf{Proportional shortage} No dummy source. Adjust the demands proportionally to match the available supply. All destinations receive the same “proportion” of their demand. For example, if the supply is $85\%$ of the total demand, each destination receives $85\%$ of its need. For each destination $D_j$ modify the demand $d_j = d_j \times \text{total supply}/\text{total demand}$
	\end{itemize}
\end{description}

\section{Assignment Problem}

The assignment problem is a special case of the transportation problem where the supply and demand are both equal to 1. The assignment problem is to find the minimum cost of assigning $n$ tasks to $n$ workers.
The decision variables are binary, $x_{ij}=1$ if task $i$ is assigned to worker $j$, and $x_{ij}=0$ otherwise. The total cost of the assignment is

\[
	\sum_{i=1}^{n}\sum_{j=1}^{n} c_{ij}x_{ij}
	.\]

In its standard form, AP is balanced (the total number of workers is equal to the total number of tasks). When not balanced, we can add dummy workers or tasks to balance the problem. Its dedicated algorithm is the Hungarian algorithm.

\subsection{LP Model}

\[
	\min Z = \sum_{i=1}^{n}\sum_{j=1}^{n} c_{ij}x_{ij}
	.\]

Subject to (with on redundant constraint)

\begin{align*}
	\sum_{j=1}^{n} x_{ij} & = 1 \quad \forall i      \\
	\sum_{i=1}^{n} x_{ij} & = 1 \quad \forall j      \\
	x_{ij}                & \geq 0 \quad \forall i,j
\end{align*}

\subsection{The Hungarian Algorithm}

\thm{}{
	The ordering by cost of feasible solutions does not change if in the cost table
	\begin{itemize}
		\ii we add or subtract a constant from all the elements of a row
		\ii we add or subtract a constant from all the elements of a column
		\ii we add or subtract a constant from all the elements of the table
	\end{itemize}
}

The Hungarian algorithm is a method for solving the assignment problem. The algorithm is as follows

\begin{enumerate}
	\ii Subtract the smallest element in each row from all the elements in that row.
	\ii Subtract the smallest element in each column from all the elements in that column.
	\ii Find the smallest number of lines that can cover all the zeros in the matrix. If the number of lines is equal to $n$, stop.
	\ii Find the smallest number not covered by the lines and subtract it from all the uncovered elements and add it to the elements that are covered by two lines.
	\ii Go to step 3.
\end{enumerate}

We then allocate the tasks to the workers according to the zeros in the matrix. If there are multiple zeros in a row or column, we allocate the task to the worker who has the least amount of zeros in his row and column.

\subsection{Special Cases}

\begin{description}
	\ii[Prohibited Assignments] If an assignment is prohibited, we can set the cost to $M$ ($\infty$).
	\ii[Unbalanced Assignment] If the number of workers is not equal to the number of tasks, we can add dummy workers or tasks to balance the problem.
	\ii[Maximization] When we have a profit table ($p_{ij}$) instead of a cost table, we can convert the problem to a minimization problem by:
	\begin{enumerate}
		\ii Find a number $k$ that is greater or equal all elements: $k \geq p_{ij}$ for all $i, j$ (we can choose $k$ as the highest profit in the matrix)
		\ii Replace each profit element $p_{ij}$ by a cost $c_{ij} = k - p_{ij}$
		\ii Solve the minimization problem
	\end{enumerate}
\end{description}

\chapter{Nonlinear Programming}

\nt{
	\begin{itemize}
		\ii If $f$ concave and has a local maximum, it is a global maximum.
		\ii If $f$ convex and has a local minimum, it is a global minimum.
	\end{itemize}
}

\section{Univariate Unconstrained Optimization}

\subsection{Bisection Method}

Given a function $f(x)$, we can find the local maximum by using the bisection method given 2 numbers $a<b$ such that $f'(a)>0>f'(b)$. The algorithm is as follows

\begin{enumerate}
	\ii Given 2 numbers $a$ and $b$.
	\ii Set $x=\frac{a+b}{2}$.
	\ii If $\frac{b-a}{2} \leq \varepsilon$, stop.
	\ii If $f'(x)>0$, set $a=x$.
	\ii Else, set $b=x$.
	\ii Repeat steps 2-5.
\end{enumerate}

\begin{algorithm}
	\SetAlgoLined
	\KwResult{Minimum of $f(x)$}
	\SetKwFunction{FMain}{Bisection}
	\SetKwProg{Fn}{Function}{:}{}
	\Fn{\FMain{$f(x),a,b,\varepsilon$}}{
		$x\gets\frac{a+b}{2}$\;
		\While{$\frac{b-a}{2}>\varepsilon$}{
			\eIf{$f'(x)>0$}{
				$a\gets x$\;
			}{
				$b\gets x$\;
			}
			$x\gets\frac{a+b}{2}$\;
		}
		\Return{$x$}\;
	}
	\caption{Bisection Method}
\end{algorithm}

\subsection{Newton's Method}

Given a function $f(x)$, we can find the local maximum by using Newton's method given a number $x_0$. This function garantees a local maximum if $f$ is concave (minimum if $f$ is convex). The algorithm is as follows

\begin{enumerate}
	\ii Given a number $x_0$.
	\ii Set $x=x_0-\frac{f'(x_0)}{f''(x_0)}$.
	\ii If $|x-x_0| \leq \varepsilon$, stop.
	\ii Set $x_0=x$.
	\ii Repeat steps 2-4.
\end{enumerate}

\begin{algorithm}
	\SetAlgoLined
	\KwResult{Minimum of $f(x)$}
	\SetKwFunction{FMain}{Newton}
	\SetKwProg{Fn}{Function}{:}{}
	\Fn{\FMain{$f(x),x_0,\varepsilon$}}{
		$x\gets x_0-\frac{f'(x_0)}{f''(x_0)}$\;
		\While{$|x-x_0|>\varepsilon$}{
			$x_0\gets x$\;
			$x\gets x_0-\frac{f'(x_0)}{f''(x_0)}$\;
		}
		\Return{$x$}\;
	}
	\caption{Newton's Method}
\end{algorithm}

\section{Multivariate Unconstrained Optimization}

\subsection{Gradient Descent}

Given a function $f(x)$, we can find the local maximum by using the gradient descent method given a number $x_0$. The function doesn't garantee a local maximum except when $f$ is convex/concave. We consider an initial guess $x_0$ and a step size $t$, then we update the guess $x_1=x_0+t\grad{f(x_0)}$ and repeat the process. We find the step size $t$ by using the exact line search method. The algorithm is as follows

\begin{enumerate}
	\ii Given a number $x_0$ and calculate $\grad{f(x_0)}$.
	\ii Maximize the univariate function $g(t)=f(x_0+t\grad{f(x_0)})$. (Exact line search)
	\ii Set $x_1=x_0+t\grad{f(x_0)}$.
	\ii If $|x_1-x_0| \leq \varepsilon$, stop.
\end{enumerate}

\begin{algorithm}
	\SetAlgoLined
	\KwResult{Minimum of $f(x)$}
	\SetKwFunction{FMain}{GradientDescent}
	\SetKwProg{Fn}{Function}{:}{}
	\Fn{\FMain{$f(x),x_0,\varepsilon$}}{
		\While{$|x_1-x_0|>\varepsilon$}{
			$t\gets\argmin_t f(x_0+t\grad{f(x_0)})$\;
			$x_1\gets x_0+t\grad{f(x_0)}$\;
			$x_0\gets x_1$\;
		}
		\Return{$x_1$}\;
	}
	\caption{Gradient Descent Method}
\end{algorithm}

\subsection{Newton's Method}

Given a function $f$ and an initial guess $x_0$

\[
	f(x_0) + \grad{f(x_0)(x-x_0)} + \frac{1}{2}(x-x_0)^T\grad^2f(x_0)(x-x_0)
	.\]

\[
	x_{i+1} = x_i - \grad^2f(x_i)^{-1}\grad{f(x_i)}
	.\]

Iterate until some convergence criteria is met $\norm{x_{i+1} - x_i} \leq \varepsilon$ or $\max_j \norm{{x_j}^{i+1} - {x_j}^i} \leq \varepsilon$.\\

The algorithm is as follows

\begin{algorithm}
	\SetAlgoLined
	\KwResult{Minimum of $f(x)$}
	\SetKwFunction{FMain}{Newton}
	\SetKwProg{Fn}{Function}{:}{}
	\Fn{\FMain{$f(x),x_0,\varepsilon$}}{
		\While{$\norm{x_{i+1} - x_i}>\varepsilon$}{
			$x_{i+1}\gets x_i - \grad^2f(x_i)^{-1}\grad{f(x_i)}$\;
			$x_i\gets x_{i+1}$\;
		}
		\Return{$x_{i+1}$}\;
	}
	\caption{Newton's Method}
\end{algorithm}

\section{Convex Sets}

A set $S$ is convex if for any $x,y\in S$ and any $\lambda\in[0,1]$, we have $\lambda x + (1-\lambda)y\in S$.

\section{Hessian Matrix and Definiteness}

The Hessian matrix is the matrix of second derivatives of a function. It is defined as

\[
	\grad^2f(x,y) = \begin{bmatrix}
		\pdv[2]{f}{x} & \pdv{f}{x}{y} \\
		\pdv{f}{y}{x} & \pdv[2]{f}{y}
	\end{bmatrix}
	.\]

\[
	f \text{ strictly convex } \Leftrightarrow \grad^2f(x) \text{ positive definite }
	.\]

\[
	H^{n \times n} \text{ positive definite } \Leftrightarrow \text{ eigenvalues} > 0 \Leftrightarrow \forall \vb{v} \in \RR^n, \vb{v}^TH\vb{v} > 0 \Leftrightarrow \text{ all leading principal minors are positive}
	.\]

\[
	f \text{ convex } \Leftrightarrow \grad^2f(x) \text{ positive semi-definite }
	.\]

\[
	H^{n \times n} \text{ positive semi-definite } \Leftrightarrow \text{ eigenvalues} \geq 0 \Leftrightarrow \forall \vb{v} \in \RR^n, \vb{v}^TH\vb{v} \geq 0 \Leftrightarrow \text{ all leading principal minors } \geq 0
	.\]

\section{Langrange Multipliers}

Given a function $f(x)$ and a set of constraints $g_i(x)=0$, we can find the local maximum by using the Langrange multipliers method. At $x^*$ the gradient of $f$ is orthogonal to the curve of the constraints.\\

The optimality conditions are

\[
	\grad{f(x^*)} = \lambda \grad{g(x^*)}
	.\]

Points that verify these conditions are called stationary points.\\

Langrange unified the constraints and the objective function into a single function $L(x,\lambda) = f(x) - \lambda g(x)$. We find the stationary points of $L$ and verify that the constraints are satisfied.\\

\[
	\pdv{L}{x} = 0 \implies \grad{f} - \lambda \grad{g} = 0 \qq*{Optimality condition}
	.\]

\[
	\pdv{L}{\lambda} = 0 \implies g(x) = 0 \qq*{Constraint}
	.\]

When there are multiple constraints $L$ is defined as

\[
	L(x,\lambda) = f(x) - \sum_i \lambda_i g_i(x) = f(x) - \vb*{\lambda}^T\vb{g}(x)
	.\]

When the constraints are inequalities $g_i(x)\leq 0$, we define the Lagrangian as

\[
	L(x,\lambda) = f(x) - \sum_i u_i g_i(x) = f(x) - \vb*{u}^T\vb{g}(x)
	.\]

The KKT conditions are

\begin{enumerate}
	\ii For $i=1,\ldots,m$
	\begin{enumerate}
		\ii $g_i(x) \leq 0$
		\ii $u_i \geq 0$
		\ii $u_ig_i(x) = 0$
	\end{enumerate}
	\ii For $j=1,\ldots,n$
	\[
		\pdv{L}{x_j} = 0
		.\]
\end{enumerate}

\nt{
	For equality constraints we drop the $u_i \geq 0$ and the $u_ig_i(x) = 0$ conditions.
}

If we have non-negativity constraints $x_i\geq 0$, we define the Lagrangian as

\[
	L(x,\lambda) = f(x) - \sum_i u_i x_i + \sum_j a_j x_j
	.\]

Where we substitute

\[
	a_j = -\pdv{f}{x_j} + \sum_i u_i \pdv{g_i}{x_j}
	.\]

The KKT conditions are

\begin{enumerate}
	\ii For $i=1,\ldots,n$
	\begin{enumerate}
		\ii $g_i \leq 0$
		\ii $u_i \geq 0$
		\ii $u_ig_i = 0$
	\end{enumerate}
	\ii For $j=1,\ldots,n$
	\begin{enumerate}
		\ii $\pdv{f}{x_j} - \sum_i u_i \pdv{g_i}{x_j} \leq 0$
		\ii $x_j(\pdv{f}{x_j} - \sum_i u_i \pdv{g_i}{x_j}) = 0$
		\ii $x_j \geq 0$
	\end{enumerate}
\end{enumerate}

\paragraph{Problem Statement}

\[
	\max Z = f(\vb{x})
	.\]

Subject to
\begin{align*}
	g_i(\vb{x}) & \leq b_i \quad i=1,\ldots,m \\
	x_j         & \leq 0 \quad j=1,\ldots,n
\end{align*}

We define the Lagrangian by adding $m$ dual variables $u_i$

\[
	L(\vb{x},\vb{u}) = f(\vb{x}) - \sum_{i=1} u_i(g_i(\vb{x}) - b_i)
	.\]

The KKT conditions are

\begin{enumerate}
	\ii For $j=1,\ldots,m$
	\begin{enumerate}
		\ii $\pdv{f}{x_j} - \sum_i u_i \pdv{g_i}{x_j} \leq 0$
		\ii $x_j(\pdv{f}{x_j} - \sum_i u_i \pdv{g_i}{x_j}) = 0$
		\ii $x_j \leq 0$
	\end{enumerate}
	\ii For $i=1,\ldots,m$
	\begin{enumerate}
		\ii $g_i(\vb{x}) - b_i \leq 0$
		\ii $u_i \geq 0$
		\ii $u_i(g_i(\vb{x}) - b_i) = 0$
	\end{enumerate}
\end{enumerate}

\nt{
	The KKT conditions are necessary conditions for optimality. They are not sufficient conditions. They are only valid for local optima. \\
	However, if $f$ is concave and the constraints are convex, then the KKT conditions are also sufficient.
}

\section{Convex Programming}

A convex programming problem is a problem where the objective function and the constraints are convex. The KKT conditions are necessary and sufficient for optimality.

\nt{
	Important properties of convex programming problems:
	\begin{itemize}
		\ii Any local extremum is a global extremum.
		\ii The KKT conditions are sufficient as well as necessary.
	\end{itemize}
}

\subsection{Quadratic Programming}

A quadratic programming problem is a convex programming problem where the objective function is quadratic and the constraints are linear. The problem has the following shape

\[
	\vb{A}\vb{x} \leq \vb{b} \quad \vb{x} \geq 0
	.\]

Quadratic polynomials can be written as

\[
	\vb{c}\vb{x} - \frac{1}{2}\vb{x}^T\vb{Q}\vb{x}
	.\]

Where $\vb{Q}$ is the Hessian matrix of the quadratic function times $-1$.

\[
	\vb{Q} = -\grad^2f(\vb{x})
	.\]

\ex{}{
	Consider the following quadratic programming problem
	\[
		\max Z = = 15x_1 + 30x_2 + 4x_1x_2 - 2{x_1}^2 - 4{x_2}^2
		.\]

	Subject to
	\[
		x_1 + 2x_2 \leq 30
		.\]

	\begin{align*}
		\vb{A} & = \begin{bmatrix}
			           1 & 2
		           \end{bmatrix}  \\
		\vb{b} & = \begin{bmatrix}
			           30
		           \end{bmatrix}  \\
		\vb{c} & = \begin{bmatrix}
			           15 \\ 30
		           \end{bmatrix}  \\
		\vb{Q} & = \begin{bmatrix}
			           4 & -4 \\ -4 & 8
		           \end{bmatrix}
	\end{align*}
}

If $Q$ is PSD (positive semi-definite), then the problem is convex. Any method would solve the problem however there exists a modified simplex method that is more efficient.

\subsection{Modified Simplex Method}

\begin{itemize}
	\ii Begin by writing down the KKT conditions.
	\ii Add slack variables $y_i, v_i$ for each inequality constraint.
	\ii Add artificial variables $z_i$ for feasibility.
	\ii Solve the problem using the simplex method with the Restricted-Entry Rule which states that we should avoid having complimentary variables in the BFS. Always select an entering basic variable that its complementary variable is not in the base (not necessarily the one that has the highest increase rate).
\end{itemize}

\ex{}{
	Consider the following quadratic programming problem
	\[
		\max Z = = 15x_1 + 30x_2 + 4x_1x_2 - 2{x_1}^2 - 4{x_2}^2
		.\]

	Subject to
	\[
		x_1 + 2x_2 \leq 30
		.\]
	The KKT conditions are
	\[
		L = 15x_1 + 30x_2 + 4x_1x_2 - 2{x_1}^2 - 4{x_2}^2 - \lambda_1(x_1 + 2x_2 - 30)
		.\]

	\begin{align*}
		15 + 4x_2 - 4x_1 - \lambda_1       & \leq 0 \\
		30 + 4x_1 - 8x_2 - 2\lambda_1      & \leq 0 \\
		x_1 + 2x_2 - 30                    & \leq 0 \\
		                                   &        \\
		x_1(15 - 4x_1 + 4x_2 - \lambda_1)  & = 0    \\
		x_2(30 + 4x_1 - 8x_2 - 2\lambda_1) & = 0    \\
		\lambda_1(x_1 + 2x_2 - 30)         & = 0    \\
	\end{align*}

	Now we add slack variables $y_1, v_1$ for the inequality constraints

	\begin{align*}
		4x_1 - 4x_2 + \lambda_1 - y_1   & = 15 \quad x_1y_1 = 0 \text{ complementarity constraint}       \\
		-4x_1 + 8x_2 + 2\lambda_1 - y_2 & = 30 \quad x_2y_2 = 0 \text{ complementarity constraint}       \\
		x_1 + 2x_2 + v_1                & = 30 \quad \lambda_1v_1 = 0 \text{ complementarity constraint} \\
	\end{align*}

	One constraint summarizes all complementarity constraints

	\[
		x_1y_1 + x_2y_2 + \lambda_1v_1 = 0
		.\]

	We add artificial variables $z_1, z_2, z_3$ for feasibility

	\[
		\min z_1 + z_2
		.\]

	\begin{align*}
		4x_1 - 4x_2 + \lambda_1 - y_1 + z_1   & = 15 \\
		-4x_1 + 8x_2 + 2\lambda_1 - y_2 + z_2 & = 30 \\
		x_1 + 2x_2 + v_1                      & = 30 \\
	\end{align*}



}


\end{document}
