\documentclass{report}

\input{../template/preamble}
\input{../template/macros}
\input{../template/letterfonts}

\title{\Huge{Statistics}\\Semester 4}
\author{\huge{}}
\date{}

\begin{document}

\maketitle
\newpage% or \cleardoublepage
% \pdfbookmark[<level>]{<title>}{<dest>}
\pdfbookmark[section]{\contentsname}{toc}
\tableofcontents
\pagebreak

\chapter{Revision of Probability}

I'm simply gonna list rules.
\begin{align*}
	\EE(X)    & = \mu =\sum_{i\in\Omega}X_i\Pr{X_i}                            \\
	\EE(g(X)) & = \sum_{i\in\Omega}g(X_i)\Pr{X_i}                              \\
	\EE(aX+b) & = a\EE(X) + b                                                  \\
	\EE(X+Y)  & = \EE(X) + \EE(Y)\quad\text{if both variables are independent}
\end{align*}

\begin{align*}
	\Var{X}     & = \sigma^2 = \EE(X^2) - \mu^2          \\
	\Var{aX+bY} & = a^2\Var{X} + b^2\Var{Y}+2ab\cov{X,Y}
\end{align*}

where
\[
	\cov{X,Y} = \EE(XY) - \EE(X)\cdot\EE(Y)
	.\]

\section{Discrete Distributions}

\begin{enumerate}
	\ii \textbf{Uniform discrete law}
	\begin{align*}
		 & X(\Omega)= \{1,2,3,\dots,n\}                        \\
		 & \Pr{X=k} = \frac{1}{n}\quad\forall k =1,2,3,\dots,n \\
		 & \begin{cases}
			\EE(X) = \frac{n+1}{2} \\
			\Var{X} = \frac{n^2-1}{12}
		\end{cases}
	\end{align*}
	\ii \textbf{Bernoulli law of parameters $p$} $(0<p<1)$
	\begin{align*}
		 & X \sim \mathrm{B}(p)              \\
		 & X(\Omega) = \{0,1\}               \\
		 & \Pr{X=1} = p \quad \Pr{X=0} = 1-p \\
		 & \begin{cases}
			\EE(X) = p \\
			\Var{X} = p(1-p)
		\end{cases}
	\end{align*}
	\ii \textbf{Binomial law of parameters $n$ and $p$}
	\begin{align*}
		 & X \sim \mathrm{Bin}(n,p)                                      \\
		 & X(\Omega) = \{1,2,\dots,n\}                                   \\
		 & \Pr{X=1} = C^k_n p^kq^{n-k}\quad\forall k\in\{0,1,2,\dots,n\} \\
		 & \begin{cases}
			\EE(X) = np \\
			\Var{X} = np(1-p)
		\end{cases}
	\end{align*}
	\ii \textbf{Hypergeometric law}
	\begin{align*}
		 & X \sim \mathcal{H}(N,n,p)                                                     \\
		 & X(\Omega) = [\max\{0,n-N+M\},\min\{M,n\}]                                     \\
		 & \Pr{X=k} = \frac{C^k_M \cdot C^{n-k}_{N-M}}{C^n_N}\quad\forall k\in X(\Omega) \\
		 & \begin{cases}
			\EE(X) = np \\
			\Var{X} = np(1-p)\lt(\frac{N-n}{N-1}\rt)
		\end{cases}
	\end{align*}
	\ii \textbf{Geometric law}
	\begin{align*}
		 & X \sim \mathrm{G}(p)                          \\
		 & X(\Omega) = \NN^*                             \\
		 & \Pr{X=k} =p(1-p)^{k-1}\quad\forall k\in \NN^* \\
		 & \begin{cases}
			\EE(X) = \frac{1}{p} \\
			\Var{X} = \frac{1-p}{p^2}
		\end{cases}
	\end{align*}
	\ii \textbf{Poisson's law of parameter} $\lambda$ $(\lambda\in\RR_+^*)$
	\begin{align*}
		 & X \sim \mathcal{P}(\lambda)                                      \\
		 & X(\Omega) = \NN                                                  \\
		 & \Pr{X=k} = e^{-\lambda}\frac{\lambda^k}{k!}\quad\forall k\in \NN \\
		 & \begin{cases}
			\EE(X) = \lambda \\
			\Var{X} = \lambda
		\end{cases}
	\end{align*}

\end{enumerate}

\section{Continuous Distributions}

\begin{enumerate}
	\ii \textbf{Uniform law}
	\begin{align*}
		 & f(x) = \begin{cases}
			\frac{1}{b-a} & \text{if } x\in[a,b] \\
			0             & \text{else}
		\end{cases} \\
		 & \begin{cases}
			\EE(x) =  \frac{a+b}{2} \\
			\Var{x} = \frac{(b-a)^2}{12}
		\end{cases}
	\end{align*}
	\ii \textbf{Continuous law}
	\begin{align*}
		 & x \sim \xi(\lambda)               \\
		 & f(x) = \begin{cases}
			\lambda e^{-\lambda x} & \text{if } x>0 \\
			0                      & \text{else}
		\end{cases} \\
		 & \begin{cases}
			\EE(x) =  \frac{1}{\lambda} \\
			\Var{x} = \frac{1}{\lambda^2}
		\end{cases}
	\end{align*}

	\ii \textbf{Normal law}
	\begin{align*}
		 & x \sim \mathcal{N}(\mu,\sigma)                                     \\
		 & f(x) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}} \\
		 & \begin{cases}
			\EE(x) =  \mu \\
			\Var{x} = \sigma^2
		\end{cases}
	\end{align*}
	For $\mathcal{N}(0,1)$
	\[
		\Phi(z) = \int_{-\infty}^z\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}\dd{x}
		.\]
	\[
		\pi(z) = \Phi(z) - 0.5 = \int_0^z\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}\dd{x}
		.\]

\end{enumerate}

\section{Convergence}

\thm{Chebyshev's inequality}{
	Let $X$ be a random variable of expectation $\EE(X)$ and variance $\Var{X}$. Then $\forall \varepsilon$
	\[
		\Pr{\lt| X-\EE(X) \rt|\geq \varepsilon} \leq \frac{\Var{X}}{\varepsilon^2}
		.\]
	it can also be stated as
	\[
		\Pr{\lt| X-\EE(X) \rt|< \varepsilon} \geq 1-\frac{\Var{X}}{\varepsilon^2}
		.\]
}

We say a sequence of random variables $X_n$ converges to $a$ $(X_n)\xrightarrow{\mathrm{Pr}}a$ if $\forall \varepsilon$
\[
	\lim_{n\to+\infty}\Pr{|X_n-a|>\varepsilon}=0
	.\]
or
\[
	\lim_{n\to+\infty}\Pr{|X_n-a|\leq\varepsilon}=1
	.\]

\thm{Weak law of large numbers}{
	Consider a random variable $(X_n)$ of mean $\mu$ and variance $\sigma^2$. Consider the random variable $\tilde{X}_n = \frac{X_1+X_2+\cdots+X_n}{n}$. It can be shown that $\tilde{X}_n$ converges to $\mu$ meaning $\forall \varepsilon$
	\[
		\lim_{n\to+\infty}\Pr{|\tilde{X}_n-\mu|>\varepsilon}=0
		.\]
}

\section{Approximations}
\thm{Binomial by a Poisson}{
	\[
		\mathrm{Bin}(n,p)\sim\mathcal{P}(np)\quad \text{if }\begin{cases}
			n\geq30  \\
			p\leq0.1 \\
			np<15
		\end{cases}
		.\]
}

\thm{Hypergeometric by a Binomial}{
	\[
		\mathcal{H}(N,n,p)\sim\mathrm{Bin}(n,p)\quad \text{if } n\leq0.05N
		.\]
}

\thm{De Moivreâ€“Laplace theorem}{
	\[
		\mathrm{Bin}(n,p)\sim\mathcal{N}\lt(np,\sqrt{np(1-p)}\rt)\quad \text{if }\begin{cases}
			n\geq30 \\
			np\geq5 \\
			n(1-p)\geq5
		\end{cases}
		.\]
	In this case the event $X=k$ can be replaced by $k-0.5<X<l+0.5$
}

\thm{Central limit theorem}{
	Let $(X_n)$ be a sequence of independent random variables following the same law of expectation $\mu$ and of standard deviation $\sigma$. Let $S_n = \sum_{i=1}^n X_i$ and $S_n^* = \frac{S_n-n\mu}{\sigma\sqrt{n}}$. It can be shown that $S_n^*$ converges in law to $\mathcal{N}(0,1)$.
	\begin{align*}
		\EE(S_n)  & = n\mu      \\
		\Var{S_n} & = n\sigma^2
	\end{align*}
}
\section{Further laws}

\thm{Chi square law}{
Let $X_1,X_2,\dots,X_n$ be $n$ independent random variables following the standard normal law $\mathcal{N}(0,1)$. Let $Y={X_1}^2 + {X_2}^2+\dots+{X_n}^2$. We say that $Y$ follows a chi-square law with $n$ degrees of freedom. $Y\sim{\chi_n}^2$.

\begin{align*}
	\EE(Y)  & = n  \\
	\Var{Y} & = 2n
\end{align*}

It can be shown that the density function of $Y$ is
\[
	f(x)=\begin{cases}
		\dfrac{1}{2^{\frac{n}{2}}\Gamma\lt(\frac{n}{2}\rt)}x^{\frac{n}{2}-1}e^{-\frac{x}{2}} & \text{if }x>0 \\
		0                                                                                    & \text{else}
	\end{cases}
	.\]
where $\Gamma$ is the gamma function
\[
	\Gamma(x)=\int_0^{+\infty}t^{x-1}e^{-t}\dd{t}\quad\forall x>0
	.\]
}

\thm{Student law(t-distribution)}{
Let $X$, $Z$ be two independent random random variables such that $X\sim\mathcal{N}(0,1)$ and $Z\sim{\chi_n}^2$. Hence the random variable
\[
	T=\frac{X}{\sqrt{\frac{Z}{n}}}
	.\]
is said to be following a student law. $T\sim\mathcal{T}_n$
\[
	f(t)=\frac{1}{\sqrt{n\pi}}\frac{\Gamma\lt(\dfrac{n+1}{2}\rt)}{\Gamma\lt(\dfrac{n}{2}\rt)}\lt(1+\frac{t^2}{n}\rt)^{-\frac{n+1}{2}}
	.\]
}

\chapter{Estimators}

Let $\theta$ be a certain characteristic of a population $P$ of $N$ individuals, for exmaple letting $\theta$ be the expectation of a certain random variable $X$ defined over the population. We take a sample of size $n<N$ of the population to estimate the value of $\theta$.\\

Let $Y_n$ be a function of the random variables $X_1,X_2,\dots,X_n$. $Y_n$ is called an estimator of $\theta$ if
\[
	\lim_{n\to+\infty}\EE(Y_n)=\theta
	.\]

a consistent estimator if
\[
	\lim_{n\to+\infty}\Var{Y_n}=0
	.\]
and an unbiased estimator if
\[
	\EE(Y_n)=\theta\quad\forall n\in\NN^*
	.\]
the value $y_n$ of $Y_n$ computed from any observed sample is called point estimation of $\theta$

\section{Point estimation of the mean}
Let $X$ be a random variable defined over the population $P$ of the expected value $\mu$ and standard deviation $\sigma$. Consider a sample $(X_1,X_2,\dots,X_n)$ of size $n$, randomly selected from $P$ such that $X_i$ are independent and follow the same law.\\

Consider the statistic $\bar{X}_n = \frac{X_1+X_2+\dots+X_n}{n}$, it is a random variable whose distribution is called the sample distribution of the mean.
\begin{align*}
	\EE(\bar{X}_n)  & = \mu                \\
	\Var{\bar{X}_n} & = \frac{\sigma^2}{n}
\end{align*}

Since $\Var{\bar{X}_n}\xrightarrow[n\to+\infty]{}0$ then $\bar{X}_n$ is a consistent unbiased estimator of the mean $\mu$.
\nt{
	The standard deviation of $\bar{X}_n$ is called standard error of the mean
	\[
		\sigma(\bar{X}_n)=\frac{\sigma}{\sqrt{n}}
		.\]
}

Due to the central limit theorem, as the sample size gets larger and larger $\bar{X}_n$ approaches a normal distribution $\bar{X}_n\sim\mathcal{N}\lt(\mu,\frac{\sigma}{\sqrt{n}}\rt)$.

\section{Point estimator of the variance}

\subsection{Suppose $\mu$ is unknown}

Consider the random variable $S^2$ (estimator of $\sigma^2$)
\[
	S^2 = \frac{1}{n}\sum_{i=1}^n \lt(X_i-\bar{X}_n\rt)^2
	.\]

The expectation of $S^2$ can be proved to be
\[
	\EE(S^2)=\frac{n-1}{n}\sigma^2
	.\]
Since $\EE(S^2)\xrightarrow[n\to+\infty]{}\sigma^2$ then $S^2$ is a biased estimator of $\sigma^2$.\\

Consider the random variable $S'^2$
\[
	S'^2 = \frac{n}{n-1}S^2 = \frac{1}{n-1}\sum_{i=1}^n \lt(X_i-\bar{X}_n\rt)^2
	.\]
Since $\EE(S'^2)=\sigma^2$ then $S'^2$ is an unbiased estimator of $\sigma^2$.\\

Hence $\sigma$ can be estimated by
\[
	S' = \sqrt{\frac{n}{n-1}}S
	.\]
and
\[
	\sigma(\bar{X}_n) = \frac{S}{\sqrt{n-1}}
	.\]
where
\begin{description}
	\ii[$\sigma^2$] variance of the population.
	\ii[$S^2$] variance of the sample.
	\ii[$\sigma^2(\bar{X}_n)$] variance of the distribution of the sample mean.
	\ii[$S'^2$] corrected variance of the sample.
\end{description}
\nt{
	It is better to estimate $\sigma^2$ using $S'^2$ than $S^2$ since $S^2$ is a biased estimator. However, if $n$ (sample size) is big enough $\lt(\frac{n}{n-1}\approx1\rt)$, then $\sigma^2$ can be estimated by $S^2$
}
\subsection{Suppose $\mu$ is known}
Consider the random variable $Z^2$ (not the variance of the sample)
\[
	Z^2=\frac{1}{n}\sum_{i=1}^n\lt(X_i-\mu\rt)^2
	.\]

Since $\EE(Z^2)=\sigma^2$ then $Z^2$ is an unbiased estimator of $\sigma^2$ thus the value $z^2 = \frac{1}{n}\sum_{i=1}^n(x_i-\mu)^2$ is a point estimation of the variance $\sigma^2$ of the population.

\nt{
	If $n>0.05N$ and if the sample is selected without replacement then the value of the variance changes to become
	\[
		\Var{\bar{X}_n}=\lt(\frac{N-n}{N-1}\rt)\frac{\sigma^2}{n}
		.\]
	and the standard error becomes
	\[
		\sigma(\bar{X}_n)=\frac{\sigma}{\sqrt{n}}\sqrt{\frac{N-n}{N-1}}
		.\]
	If the variance of the population is not known then we can use use $S^2$ or $Z^2$ to estimate $\Var{\bar{X}_n}$
	\[
		\Var{\bar{X}_n}=\lt(\frac{N-n}{N-1}\rt)\frac{S^2}{n-1}
		.\]
	and the standard error with
	\[
		\sigma(\bar{X}_n)=\frac{S}{\sqrt{n-1}}\sqrt{\frac{N-n}{N-1}}
		.\]
}

\section{Point estimation of a proportion (percentage)}

Consider a population $P$ of individuals with a proportion $p$ if individuals having a certain characteristic $\theta$. Let $(a_1,a_2,\dots,a_n)$ be a sample randomly selected $P$. We define for each individual $a_i$ the Bernoulli random variable $X_i$ as follows
\[
	\begin{cases}
		X_i=1 & \text{if } a_i \text{ has the characteristic }\theta \text{ with probability }p \\
		X_i=0 & \text{else}
	\end{cases}
	.\]

Let $Y_n=\frac{X_1+X_2+\dots+X_n}{n}=\frac{1}{n}\sum_{i=1}^nX_i$. $Y_n$ is the random variable giving the proportion of individuals of the sample that have the characteristic $\theta$.
\begin{align*}
	\Pr{X_i=1} & =\frac{\text{number of individuals of the population having }\theta}{\text{total number of individuals}} = p \\
	\Pr{X_i=0} & =1-p
\end{align*}

Thus $X_1+X_2+\dots+X_n\sim\mathrm{Bin}(n,p)$
\begin{align*}
	\EE(X_1+X_2+\dots+X_n)  & = np      \\
	\Var{X_1+X_2+\dots+X_n} & = np(1-p)
\end{align*}
\begin{align*}
	\EE(Y_n)  & = p                \\
	\Var{Y_n} & = \frac{p(1-p)}{n}
\end{align*}

Hence $Y_n$ is a consistent unbiased estimator of $p$. Therefore any observed value $y_n$ of $Y_n$ is a point estimator of $P$, meaning $p$ is estimated by the proportion of the sample.

\section{Confidence interval}

\subsection{Confidence interval for the mean}

\begin{enumerate}

	\ii \textbf{Suppose that $n\geq30$, the population is normally distributed, and $\sigma$ is known}\\
	Let $X$ be a random variable defined over a population $P$ of mean $\EE(X)=\mu$ and of variance $\Var{X}=\sigma^2$.

	Here we consider that $\bar{X}_n\sim\mathcal{N}\lt(\mu,\frac{\sigma}{\sqrt{n}}\rt)$. Hence $\frac{\bar{X}_n-\mu}{\sigma_{\bar{X}_n}}\sim\mathcal{N}(0,1)$.

	Given the probability $\gamma$ (level of confidence), we can find $t$ such that
	\begin{align*}
		\Pr{-t\leq\frac{\bar{X}_n-\mu}{\sigma_{\bar{X}_n}}\leq t}                   & =\gamma \\
		\Pr{\bar{X}_n-t\sigma_{\bar{X}_n}\leq\mu\leq \bar{X}_n+t\sigma_{\bar{X}_n}} & =\gamma
	\end{align*}
	where $\pi(t)=\frac{\gamma}{2}$. Knowing $\gamma$ we get $t$. Therefore a $\gamma\%$ confidence interval for the mean $\mu$ is given by
	\[
		\mathrm{IC}_\gamma(\mu)=[\bar{x}_n-t\sigma_{\bar{X}_n},\bar{x}_n+t\sigma_{\bar{X}_n}]
		.\]

	where
	\[
		\sigma_{\bar{X}_n} = \begin{cases}
			\frac{\sigma}{\sqrt{n}} & \text{if }\sigma\text{ is known}                                           \\
			\frac{S}{\sqrt{n-1}}    & \text{if }\sigma\text{ is unknown (estimated by }S'=\sqrt{\frac{n}{n-1}}S) \\
		\end{cases}
		.\]
	\ii \textbf{Suppose that $n<30$, the population is normally distributed, and $\sigma$ is unknown}:\\

	Using the table of student distributed knowing $\gamma$, we determine $t$ such that
	\[
		\Pr{\bar{X}_n-t\frac{S}{\sqrt{n-1}}\leq \mu\leq \bar{X}_n+t\frac{S}{\sqrt{n-1}}} = \gamma
		.\]

	hence the confidence inteval for the mean $\mu$ is
	\[
		\mathrm{IC}_\gamma(\mu) = \lt[\bar{X}_n-t\frac{S}{\sqrt{n-1}},\bar{X}_n+t\frac{S}{\sqrt{n-1}}\rt]
		.\]

	\thm{}{
		\begin{enumerate}
			\ii $\bar{X}_n$ and $S^2$ are two independent random variance.
			\ii The random variable $n\frac{S^2}{\sigma^2}$ follows a chi-square law with $n-1$ degrees of freedom.
		\end{enumerate}
	}

	\thm{}{
		The random variable
		\[
			\tilde{T} = \frac{\bar{X}_n-\mu}{\frac{S'}{\sqrt{n}}} = \frac{\bar{X}_n-\mu}{\frac{S}{\sqrt{n-1}}}
			.\]
		follows a student law (t-distribution) with $n-1$ degrees of freedom
	}
	\ii \textbf{Suppose that $n<30$, the population is not normally distributed}:\\
	In this case we cannot use the normal distributed nor the student distribution. However we can use Chebyshev's inequality.
	\[
		\Pr{|\bar{X}_n-\mu|\leq\varepsilon}\geq 1 -\frac{\sigma_{\bar{X}_n}^2}{\varepsilon^2}
		.\]
	Take $\varepsilon=t\sigma_{\bar{X}_n}$
	\[
		\Pr{\bar{X}_n-t\sigma_{\bar{X}_n} \leq \mu \leq \bar{X}_n+t\sigma_{\bar{X}_n}} \geq 1-\frac{1}{t^2}
		.\]
	Then we set $1-\frac{1}{t^2}$ equal to $\gamma$ solve for $t$ and find the interval as follows
	\[
		\mathrm{IC}_\gamma = [\bar{x}_n-t\sigma_{\bar{X}_n},\bar{x}_n+t\sigma_{\bar{X}_n}]
		.\]
	\nt{
		\begin{itemize}
			\ii if $\sigma$ is known then $\sigma_{\bar{X}_n}=\frac{\sigma}{\sqrt{n}}$
			\ii if $\sigma$ is unknown then we replace $\sigma_{\bar{X}_n}$ by its point estimator $\frac{S'}{\sqrt{n}}=\frac{S}{\sqrt{n-1}}$
		\end{itemize}
	}
\end{enumerate}

\subsection{Confidence interval for a proportion (precentage)}

same setup as last time. If we assume this time that $\mathrm{Bin}(n,p)\approx \mathcal{N}\lt(np,\sqrt{np(1-p)}\rt)$ if ($n\geq30,\;np,n(1-p)\geq5$) then we can say $Y_n\sim\mathcal{N}\lt(p,\sqrt{\frac{p(1-p)}{n}}\rt)$. Knowing $\gamma$ we can determine $t$ such that
\[
	\Pr{-t\leq \frac{Y_n-p}{\sigma_{Y_n}}\leq t}=\gamma
	.\]
The confidence interval becomes
\[
	[y_n-t\sigma_{Y_n}, y_n+t\sigma_{Y_n}]
	.\]

where $\sigma_{Y_n}=\sqrt{\frac{p(1-p)}{n}}$ estimated by
\[
	\sqrt{\frac{n}{n-1}}\sqrt{\frac{y_n(1-y_n)}{n}}=\sqrt{\frac{y_n(1-y_n)}{n-1}}
	.\]

Therefore the confidence interval becomes
\[
	\mathrm{IC}_\gamma(p)=\lt[y_n-t\sqrt{\frac{y_n(1-y_n)}{n-1}}, y_n+t\sqrt{\frac{y_n(1-y_n)}{n-1}}\rt]
	.\]

\nt{
	If $n\geq 100$ then $\frac{n}{n-1}\approx 1$, then the confidence interval s
	\[
		[y_n-t\sqrt{\frac{y_n(1-y_n)}{n}},y_n+t\sqrt{\frac{y_n(1-y_n)}{n}}]
		.\]
}
\nt{
	If the sample is selected without replace and if $n>0.05N$ then we shall put a correcting factor $\frac{N-n}{N-1}$ to $\sigma_{Y_n}=\sqrt{\frac{p(1-p)}{n}}$, thus the confidence interval for proportion $p$ becomes
	\[
		\lt[ y_n-t\sqrt{\frac{N-n}{N-1}}\sqrt{\frac{y_n(1-y_n)}{n-1}}, y_n +t\sqrt{\frac{N-n}{N-1}}\sqrt{\frac{y_n(1-y_n)}{n-1}} \rt]
		.\]
}
\subsection{Confidence interval for the variance}
Assume $X\sim\mathcal{N}(\mu,\sigma)$ and $X_1,X_2,\dots,X_n$ $n$ independent random variables and identically distributed as $X$. We set the variables
\begin{align*}
	\bar{X}_n & = \frac{1}{n}\sum_{i=1}^nX_i                                          \\
	S^2       & = \frac{1}{n}\sum_{i=1}^n\lt(X_i-\bar{X}_n\rt)^2                      \\
	S'^2      & = \frac{n}{n-1}S^2 = \frac{1}{n-1}\sum_{i=1}^n\lt(X_i-\bar{X}_n\rt)^2 \\
	Z^2       & =  \frac{1}{n}\sum_{i=1}^n\lt(X_i-\mu\rt)^2                           \\
\end{align*}

we have
\begin{align*}
	\bar{X}_n             & \sim\mathcal{N}\lt(\mu,\frac{\sigma}{\sqrt{n}}\rt) \\
	n\frac{S^2}{\sigma^2} & \sim{\chi_{n-1}}^2                                 \\
	n\frac{Z^2}{\sigma^2} & \sim{\chi_n}^2                                     \\
\end{align*}

\begin{enumerate}
	\ii Suppose $\mu$ is unknown\\

	Since $n\frac{S^2}{\sigma^2}\sim{\chi_{n-1}}^2$, then we determine the values $v_{\alpha/2}$ and $v_{1-\alpha/2}$ from the chi-square table such that
	\[
		\Pr{v_{\alpha/2}\leq \frac{nS^2}{\sigma^2}\leq v_{1-\alpha/2}} = \gamma=1-\alpha
		.\]
	therefore a confidence interval of level $\gamma$ (risk $\alpha$) is given by
	\[
		IC_\gamma(\sigma^2)=\lt[\frac{nS^2}{v_{1-\alpha/2}}, \frac{nS^2}{v_{\alpha/2}} \rt] = \lt[\frac{(n-1)S'^2}{v_{1-\alpha/2}}, \frac{(n-1)S'^2}{v_{\alpha/2}} \rt]
		.\]

	\ii Suppose $\mu$ is known\\

	From the chi-square table, we determine the values of the quantities $v_{\alpha/2}$ and $v_{1-\alpha/2}$ for the law ${\chi_n}^2$ such that
	\[
		\Pr{v_{\alpha/2}\leq \frac{nZ^2}{\sigma^2}\leq v_{1-\alpha/2}}=\gamma
		.\]

	Therefore the confidence interval of level $\gamma$ is given by
	\[
		\mathrm{IC}_\gamma(\sigma^2)=\lt[\frac{nz^2}{v_{1-\alpha/2}},\frac{nz^2}{v_{\alpha/2}}\rt]
		.\]

\end{enumerate}

\end{document}
