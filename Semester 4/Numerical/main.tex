\documentclass{report}

\input{../template/preamble}
\input{../template/macros}
\input{../template/letterfonts}

\title{\Huge{Numerical Analysis}\\Semester 4}
\author{}
\date{}

\begin{document}

\maketitle
\newpage% or \cleardoublepage
% \pdfbookmark[<level>]{<title>}{<dest>}
\pdfbookmark[section]{\contentsname}{toc}
\tableofcontents
\pagebreak

\chapter{Interpolation}

\section{Linear Interpolation}

\begin{minipage}{0.4\linewidth}
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}
			\begin{axis}[
					legend pos=outer north east,
					axis lines = box,
					xlabel = $x$,
					ylabel = $f(x)$,
					variable = t,
					trig format plots = rad,
					axis x line=bottom,
					axis y line=left,
					xmin=0,xmax=5.5,
					ymin=0,ymax=3.9,
				]

				\addplot[
					color=blue,
				]
				coordinates {
						(0,0)(1,2)(2,3)(3,1)(4,3)(5,3.5)
					};
				\addplot[
					color=red,
					only marks,
					mark=*,
				]
				coordinates {
						(0,0)(1,2)(2,3)(3,1)(4,3)(5,3.5)
					};

			\end{axis}
		\end{tikzpicture}
	\end{figure}
\end{minipage}
\hfill
\begin{minipage}{0.4\linewidth}
	\begin{tabular}{l|l}
		$x$ & $f(x)$ \\
		\hline
		0   & 0      \\
		1   & 2      \\
		2   & 3      \\
		3   & 1      \\
		4   & 3      \\
		5   & 3.5    \\
	\end{tabular}
\end{minipage}

Linear interpolation is just drawing lines between the data points.
\dfn{Linear Interpolation(lerp) equation}{
	The equation of the lines between data points is

	\[
		y = \frac{y_{i+1} - y_i}{x_{i+1} - x_i}(x-x_i) + y_i
		.\]
}
\thm{Error due to linear interpolation}{
	Let $f$ be a continuous and differentiable on $[a,b]$. We define the error $z(x)$ to be
	\[
		|z(x)| \leq \frac{(b-a)^2 }{8 }\sup_{a\leq x \leq b}\lt|f''(x)\rt|
		.\]
}

\section{Polynomial Interpolation}

\subsection{Lagrange Polynomials}

Really nice video \href{https://www.youtube.com/watch?v=bzp_q7NDdd4}{here} explaining Lagrange polynomials.


\begin{figure}[H]
	\centering
	\begin{tikzpicture}
		\begin{axis}[
				legend pos=outer north east,
				axis lines = box,
				xlabel = $x$,
				ylabel = $f(x)$,
				axis x line=bottom,
				axis y line=left,
				xmin=0,xmax=5.5,
			]

			\addplot[
				only marks,
				color=red,
				mark=*,
			]
			coordinates {
					(0,0)(1,2)(2,3)(3,1)(4,3)(5,3.5)
				};

			\addplot [
				domain=0:5,
				samples=100,
				color=blue,
			]
			{-0.17916*x^5+(13/6)*x^4+(-425/48)*x^3+(163/12)*x^2+(-283/60)*x};
			% \addlegendentry{$-\frac{43}{240}x^5 + \frac{13}{6}x^4 - \frac{425}{48}x^3 + \frac{163 }{12}x^2 - \frac{283}{60}x$}
		\end{axis}
	\end{tikzpicture}
\end{figure}


\thm{Lagrange polynomial equation}{
	Consider a set of $n$ points $(x_1,y_1),(x_2,y_2),\dots,(x_n,y_n)$. The Lagrange polynomial for this set of data is
	\[
		L(x) = \sum_{k=0}^{n} y_k \ell_k(x)
		.\]

	where
	\[
		\ell_k(x) = \prod_{\substack{i=1\\i\neq k}}^{n}\frac{x-x_i }{x_k-x_i}
		.\]
}

\subsubsection{Case of equidistant points}

If the set of $x_i$ are equidistant from each other with a distance of $h=x_{i+1}-x_i$, then we can represent any point as $x_k = x_0 + kh$ where $k\in\mathbb{N}$ and any number $x = x_0 + sh$ where $s\in\mathbb{R}$. We can rewrite the formula as
\[
	Q(s) = \sum_{k=0}^{n} \ell_k(s)f(x_k)
	.\]
where
\[
	\ell_k(s) = \prod_{\substack{j=0\\j\neq k}}^n\frac{s-j}{k-j}
	.\]

by substitution
\[
	s = \frac{x-x_0}{h}
	.\]

\subsubsection{Existence}

\begin{myproof}
	$P(x)$ belongs to the vectorial space of polynomial of degree of, at most, $n$. Now, we must fins a basis for this vectorial space. Find the polynomial $\ell_k$ of degree $\leq n$ such that
	\[
		\ell_k(x_i) = \delta_{ki} = \begin{cases}
			1 & \text{if }i=k     \\
			0 & \text{if }i\neq k
		\end{cases}
		.\]
	Then, $\ell_k(x) = \lambda(x-x_0)\dots(x-x_{k-1})(x-x_{k+1})\dots(x-x_n)$
	where
	\[
		\lambda = \frac{1}{(x_k-x_0)\dots(x_k-x_{k-1})(x_k-x_{k+1})\dots(x_k-x_n)}
		.\]
	The $(n+1)$ polynomials $\ell_k(x)$ for a system of generators in the vectorial space of polynomials of degree at most $n$.

	\[
		\lambda_0\ell_0(x) + \lambda_1\ell_1(x) + \dots + \lambda_k\ell_k(x) + \dots + \lambda_n\ell_n(x) = 0
		.\]

	for $x=x_k$
	\begin{align*}
		\lambda_0\ell_0(x_k) + \lambda_1\ell_1(x_k) + \dots + \lambda_k\ell_k(x_k) + \dots + \lambda_n\ell_n(x_k) & = 0 \\
		0+0+\dots+\lambda_k1+\dots+0                                                                              & = 0 \\
		\lambda_k                                                                                                 & = 0
		.\end{align*}
	$\therefore$ the set of $\ell_k$ for a basis in the vector space $\Rightarrow$ there has to exist a polynomial passing through the given set of points.

\end{myproof}


\subsubsection{Uniqueness}

\begin{myproof}
	Let $P$ and $Q$ be 2 Lagrange polynomials of degrees $\leq n\Big/P(x_i)=Q(x_i)=f(x_i)\quad\forall i = 0,1,\dots,n$.\\
	Let
	\[
		\begin{rcases}
			R=P-Q \text{ of degree }\leq n \\
			R=0 \; (n+1) \text{ times}
		\end{rcases}R\equiv0\Rightarrow P=Q\;\forall x
		.\]
\end{myproof}

\subsection{Newton Polynomial}

\dfn{Newton Polynomial equation}{
	Consider a set of $n$ points $(x_1,y_1),(x_2,y_2),\dots,(x_n,y_n)$. The Newton polynomial for this set of data is
	\[
		p_n(x) = \underbrace{a_0}_{A_0} + \underbrace{a_1(x-x_0)}_{A_1} + \underbrace{a_2(x-x_0)(x-x_1)}_{A_2} + \dots + \underbrace{a_n \prod_{i=0}^{n-1}(x-x_i)}_{A_n}
		.\]
	where
	\[
		a_i = f[x_0,x_1,\dots,x_i]
		.\]
	Here $f[\dots]$ is the divided difference of the inputted data.
}

\dfn{Backwards formula}{
\[
	P_n(x) = f_n + A_1 + A_2 + \dots+ A_n
	.\]

where
\[
	A_i = f[x_n,x_{n-1},\dots,x_{n-i}]\prod_{j=n-i+1}^{n}(x-x_j)
	.\]
}

The divided difference has 2 formulas, the recurrence formula
\[
	f[x_0,x_1,\dots,x_{n+1}] = \frac{f[x_1,x_2,\dots,x_{n+1}]-f[x_0,x_1,\dots,x_{n}]}{x_{n+1}-x_0}
	.\]

and a general formula
\[
	f[x_0,x_1,\dots,x_n] = \sum_{i=1}^{n}\frac{y_i }{\prod_{\substack{k=0\\k\neq i}}^{n} (x_i - x_k)}
	.\]
Now forget you ever saw those cause there is an easier method to finding the divided difference.

\subsubsection{Divided Difference Table}
\[
	\begin{array}{cccccc}
		x_0 & y_0                                                                                              \\
		    &     & \frac{y_1-y_0}{x_1-x_0}=f[x_0,x_1]                                                         \\
		x_1 & y_1 &                                    & \frac{f[x_1,x_2]-f[x_0,x_1]}{x_2-x_0}                 \\
		    &     & \frac{y_2-y_1}{x_2-x_1}=f[x_1,x_2] &                                       & \dots         \\
		x_2 & y_2 &                                    & \frac{f[x_2,x_3]-f[x_1,x_2]}{x_3-x_1} &       & \dots \\
		    &     & \frac{y_3-y_2}{x_3-x_2}=f[x_2,x_3] &                                       & \dots         \\
		x_3 & y_3 &                                    & \frac{f[x_3,x_4]-f[x_2,x_3]}{x_4-x_2}                 \\
		    &     & \frac{y_4-y_3}{x_4-x_3}=f[x_3,x_4]                                                         \\
		x_4 & y_4
	\end{array}
\]
After we have constructed the table we can find the divided difference we want by looking at the top diagonal

\[
	\begin{array}{cccccc}
		x_0 & y_0                                                                             \\
		    &     & f[x_0,x_1]                                                                \\
		x_1 & y_1 &            & f[x_0,x_1,x_2]                                               \\
		    &     & \dots      &                & f[x_0,x_1,x_2,x_3]                          \\
		x_2 & y_2 &            & \dots          &                    & f[x_0,x_1,x_2,x_3,x_4] \\
		    &     & \dots      &                & \dots                                       \\
		x_3 & y_3 &            & \dots                                                        \\
		    &     & \dots                                                                     \\
		x_4 & y_4
	\end{array}
\]


\subsubsection{Case of equidistant points}
Bla bla bla the formula becomes

\[
	P(t) = a_0 + a_1(t-0) + a_2(t-0)(t-1) + \dots + a_n \prod_{i=0}^{n-1}(t-i)
	.\]

where in this case
\[
	a_k = \frac{\nabla^k[y](x_k)}{k!}
	.\]

and
\[
	x = x_0 +th
	.\]

where $\nabla^k[y]$ is the discrete difference.

\[
	\nabla[y](x_i) = y(x_i+h)-y(x_i)
	.\]

and the backwards formula is
\[
	P(t) = f_n + A_1 + A_2 + \dots + A_n
	.\]

where
\[
	A_i = \frac{\bar{\nabla}^if_n}{i!}\prod_{j=n-i+1}^n (t-j)
	.\]

\dfn{Discrete Difference}{
	Forward discrete difference:
	\begin{align*}
		\nabla[y](x_i)   & = y(x_i+h) - y(x_i)                    \\
		\nabla^2[y](x_i) & = \nabla[y](x_i + h) - \nabla [y](x_i) \\
		                 & = y(x_i+2h)-2y(x_i+h)+y(x_i)           \\
		\nabla^k[y](x_i) & = \nabla\lt(\nabla^{k-1}[y](x_i)\rt)
	\end{align*}


	Backwards discrete difference:
	\begin{align*}
		\bar{\nabla}[y](x_i)   & = y(x_i) - y(x_i - h)                            \\
		\bar{\nabla}^k[y](x_i) & = \bar{\nabla}\lt(\bar{\nabla}^{k-1}[y](x_i)\rt)
	\end{align*}
}
\[
	\begin{array}{cccccc}
		x_0 & y_0                                                                           \\
		    &     & \nabla[y](x_i)                                                          \\
		x_1 & y_1 &                & \nabla^2[y](x_i)                                       \\
		    &     & \dots          &                  & \nabla^3[y](x_i)                    \\
		x_2 & y_2 &                & \dots            &                  & \nabla^4[y](x_i) \\
		    &     & \dots          &                  & \dots                               \\
		x_3 & y_3 &                & \dots                                                  \\
		    &     & \dots                                                                   \\
		x_4 & y_4
	\end{array}
\]

\subsection{Error due to polynomial interpolation}

Let $f(x)$ be of class $C^{n+1} \quad \forall x \in [a,b]$ and let the polynomial $P(x)$ interpolate it. \\

The error function is bounded by
\[
	|\text{Error}| = |f(x) - P(x)| \leq \frac{\lt| \prod_{i=0}^n (x-x_i) \rt|}{(n+1)!} \sup_{x\in[a,b]} \lt|f^{(n+1)}(x)\rt|
	.\]

\subsection{Hermite Interpolation}

\dfn{Hermite interpolation formula}{
	Consider $(n+1)$ sets of point $(x_i,y_i,y'_i)$ representing $f(x)$ ($y_i = f(x_i)$ and $y_i' = f'(x_i)$), the hermite polynomial $P(x)$ interpolates $f(x)$ such that $P'(x) = f'(x)$.

	\[
		P(x) = \sum_{i=0}^n h_i(x)y_i + \sum_{i=0}^{n}k_i(x)y_i'
		.\]

	where
	\begin{align*}
		h_i(x)    & =\lt(1-2(x-x_i)\ell_i'(x_i)\rt)\ell^2_i(x) \\
		k_i(x)    & =(x-x_i)\ell^2_i(x)                        \\
		\ell_i(x) & = \prod_{\substack{j=0                     \\j\neq i}}^n \frac{x-x_j}{x_i-x_j}
	\end{align*}
}

\thm{Error due to Hermite interpolation}{
\[
	|\text{Error}| = |f(x) - P(x)| \leq \frac{\lt| \prod_{i=0}^n (x-x_i)^2 \rt|}{(2n+2)!} \sup_{x\in[a,b]} |f^{(2n+2)}(x)|
	.\]
}

\subsubsection{Existence}

\begin{myproof}

	\[
		P(x) = \sum_{i=0}^n h_i(x)y_i + \sum_{i=0}^{n}k_i(x)y_i'
		.\]

	where
	\begin{align*}
		h_i(x)    & =\lt(1-2(x-x_i)\ell_i'(x_i)\rt)\ell^2_i(x) \\
		k_i(x)    & =(x-x_i)\ell^2_i(x)                        \\
		\ell_i(x) & = \prod_{\substack{j=0                     \\j\neq i}}^n \frac{x-x_j}{x_i-x_j}
	\end{align*}

	Let $i\neq j$.
	\begin{align*}
		k_i(x_j) & = (x_j-x_i)\ell_i^2(x_j)=0 \\
		k_i(x_i) & = (x_i-x_i)\ell_i^2(x_i)=0
	\end{align*}
	and
	\begin{align*}
		h_i(x_j) & = (1-2(x_j-x_i)\ell_i'(x_i))\ell_i^2(x_j)=0 \\
		h_i(x_j) & = (1-2(x_i-x_i)\ell_i'(x_i))\ell_i^2(x_i)=1
	\end{align*}
	We conclude that $P(x_i) = f(x_i)$\\

	Now we have to prove that $P'(x_i) = f'(x_i)$
	\begin{align*}
		h_i'(x) & = -2\ell_i'(x_i)\ell_i^2(x)+2(1-2(x-x_i)\ell_i'(x_i))\ell_i(x)\ell_i'(x) \\
		k_i'(x) & = \ell^2_i(x) + 2(x-x_i)\ell_i(x)\ell_i'(x)
	\end{align*}

	\begin{align*}
		h_i'(x_j) & = -2\ell_i'(x_i)\ell_i^2(x_j)+2(1-2(x_j-x_i)\ell_i'(x_i))\ell_i(x_j)\ell_i'(x_j) = 0 \\
		h_i'(x_i) & = -2\ell_i'(x_i)\ell_i^2(x_i)+2(1-2(x_i-x_i)\ell_i'(x_i))\ell_i(x_i)\ell_i'(x_i) =0  \\
	\end{align*}

	\begin{align*}
		k_i'(x_j) & = \ell^2_i(x-j) + 2(x_j-x_i)\ell_i(x_j)\ell_i'(x_j) = 0 \\
		k_i'(x_j) & = \ell^2_i(x-i) + 2(x_i-x_i)\ell_i(x_i)\ell_i'(x_i) = 1 \\
	\end{align*}
	$\therefore P'(x_i) = f'(x_i)$
\end{myproof}

\subsubsection{Uniqueness}

\begin{myproof}
	Suppose that there exists 2 polynomials $P$ and $Q$ of degree $n\leq 2n+1$ such that $P(x_i)=Q(x_i)=f(x_i)$ and $P'(x_i)=Q'(x_i)=f'(x_i)\;\forall i = 0,1,\dots,n$.\\
	Let $R(x)=P(x)-Q(x)$.\\
	$R=0\;(n+1)$ times $\Rightarrow$ according to Rolle's theorem $\exists\,n$ points $\neq x_i\Big/R'=0$\\
	$R'=0\;n$ times as a consequence of Rolle's theorem then
	\[
		\begin{rcases}
			R'(x)=0\;(2n+1)\text{ times} \\
			R'(x)\text{ is of degree }2n
		\end{rcases}
		R'(x)=0 \;\forall x
		.\]

	\[
		R'(x)=0\Rightarrow R(x)=\text{cnst}\quad\text{and}\quad R(x_i)=P(x_i)-Q(x_i)=0 \Rightarrow \text{cnst } = 0
		.\]

	\[
		R(x) = P(x)-Q(x)=0 \;\forall x
		.\]
	$\therefore P(x) = Q(x)$
\end{myproof}


\chapter{Finding $f(x)=0$}

We will assume that every function is defined in the interval $I = [a,b]$ and that every $x_0\in I$

\section{Bisection Method}

Suppose that $f$ is a continuous and monotone function on the domain $I = [a,b]$ such that $f(a)f(b)<0\Rightarrow \exists r \in]a,b[:\;f(r)=0$.\\

At each step in the algorithm, in an iteration we let $c = (a+b)/2$, then we check the value of $f(c)$, if it is 0 then we are done.\\
However when it is not, then we define a new interval such that

\[
	I = \begin{cases}
		[a,c] & \text{if }f(c)f(a)<0 \\
		[c,b] & \text{if }f(c)f(b)<0
	\end{cases}
	.\]

We repeat this step until the length of the interval reaches a certain number (for example $|b-a|<10^{-5}$), at this point we stop and the best guess for the root would be $(a+b)/2$

\subsubsection{Error of the Bisection Method}

After $n$ iterations, the error of the approximated root would be
\[
	\text{Error} \leq \frac{|b-a|}{2^{n+1}}
	.\]

\section{Lagrange Method}

Suppose that $f$ is a continuous and monotone function on the domain $I = [a,b]$ such that $f(a)f(b)<0\Rightarrow \exists r \in]a,b[:\;f(r)=0$.\\

The starting value of $x_0$ depends on the value of $f$
\[
	x_0 = \begin{cases}
		a & \text{if }f(a)f''(a)<0 \\
		b & \text{if }f(b)f''(b)<0
	\end{cases}
	.\]

then we can find a new guess $x$ depending on the value of $x_0$
\begin{itemize}
	\ii if $x_0 = a$

	\[
		x_1 = x_0 - \frac{b-x_0}{f(b)-f(x_0)}f(x_0)
		.\]
	\ii if $x_0 = b$
	\[
		x_1 = x_0-\frac{a-x_0}{f(a)-f(x_0)}f(x_0)
		.\]
\end{itemize}

\subsubsection{Error from Lagrange Method}
For the first iteration
\[
	\text{Error} \leq \sup_{x\in[a,b]}|f''(x)|\frac{(b-a)^2}{8}
	.\]
For the second iteration
\[
	M_2 = \sup_{x\in[a,b]}|f''(x)|
	.\]
\begin{itemize}
	\ii if $x_0 = a$
	\[
		\text{Error} \leq \frac{M_2}{8}\lt|\frac{(b-x_0)^3}{f(b)-f(x_0)}\rt|
		.\]
	\ii if $x_0 = b$
	\[
		\text{Error} \leq \frac{M_2}{8}\lt|\frac{(a-x_0)^3}{f(a)-f(x_0)}\rt|
		.\]
\end{itemize}

\section{Newton Method}

Suppose that $f$ is a continuous and monotone function on the domain $I = [a,b]$ such that $f(a)f(b)<0\Rightarrow \exists r \in]a,b[:\;f(r)=0$.\\

The starting value of $x_0$ depends on the value of $f$
\[
	x_0 = \begin{cases}
		a & \text{if }f(a)f''(a)>0 \\
		b & \text{if }f(b)f''(b)>0
	\end{cases}
	.\]

Then the new guess for the root would be
\[
	x_1 = x_0 - \frac{f(x_0)}{f'(x_0)}
	.\]

\subsection{Improved Newton Method}
To improve the method we first let $\eta = b-a$, and we define condition
\[
	\frac{\eta M_2}{2|f'(x_0)|}<1
	.\]
if the condition is not satisfied we need to choose another interval $[a_1,b_1]\subset I$ where $f(a_1)f(b_1)<0$

\subsubsection{Error due to Newton Method}

For one iteration
\[
	\text{Error} = \leq \frac{\eta^2 M_2}{2|f'(x_0)|} \quad\text{where}\quad M_2 = \sup_{x\in[x_0-\eta,x_0+\eta]} |f''(x)|
	.\]

\section{Fixed Point Iteration Method}
If a function can be converted to the form $x=g(x)$ along with the sequence $x_{n+1} = g(x_n)$ with initial guess $x_0$, then it is called a fixed point scheme.

The scheme converges if
\begin{itemize}
	\ii $\forall x \in [a,b]:\; g(x)\in[a,b]$
	\ii $g$ is strictly contracting meaning that $\exists \varepsilon \in \RR\; 0\leq \varepsilon < 1$
	\[
		\forall x,y \in[a,b],\; |g(x)-g(y)|\leq \varepsilon|x-y|
		.\]
\end{itemize}
then $\forall x_0$ the sequence converges to $l\in[a,b]$

\nt{
	\[
		\sup_{x\in[a,b]}|g'(x)| = L<1\Rightarrow g(x)\text{ is strictly contracting}
		.\]
}

\nt{
	Let $l$ be the solution to $g(l)=l$
	\begin{itemize}
		\ii If $|g'(l)| <1$ then there exists an interval $I$ containing $l$ for which the sequence converges to $l$
		\ii If $|g'(l)| >1$ then the sequence diverges
	\end{itemize}
}

\section{Order of Convergence}
Order of convergence (Rate of convergence) tells us how the error decreases between 2 iterations. The order of convergence $p$ of a sequence is defined to be
\[
	\lim_{n\to+\infty}\lt|\frac{x_{n+1}-l}{(x_n-l)}\rt|\in \RR^*_+
	.\]

\nt{
	The order of convergence of
	\begin{itemize}
		\ii Lagrange Method
		\[
			g'(l) = \frac{(b-l)^2}{2f(b)}f''(c)
			.\]
		If $f''(c)\neq0$ then $g'(l)\neq0$ then the order is 1.
		\ii Newton method, if $g'(l)=0$ then the order is at least 2.
	\end{itemize}
}

\nt{
	We stop the iteration method when
	\begin{itemize}
		\ii First case $g'(x)<0$, then we stop iteration when
		\[
			|x_{n+1}-r|<\varepsilon
			.\]
		\ii Second case $g'(x)>0$, then we stop iteration when
		\[
			|f(x_n)|<\eta
			.\]
		where
		\[
			\eta = \varepsilon\inf|f'(x)|
			.\]
	\end{itemize}
}

\section{Polynomial Shenanigans}
\subsection{Roots of $x^3 + px + q = 0$}

Let $y_1(x) = x^3 + px$ and $y_2(x)=-q$

\begin{itemize}
	\ii $p\geq 0\Rightarrow \exists\,1$ root
	\ii $p<0$ then we have 3 separate cases
	\[
		27q^2 + 4p^3 \begin{cases}
			=0 & \text{we have 2 separate real roots (one double and one single)} \\
			>0 & \text{we have one real root}                                     \\
			<0 & \text{we have 3 separate real roots}
		\end{cases}
		.\]
\end{itemize}

\subsection{Roots of $x^3+ax^2 + bx +c = 0$}

If we replace $x$ with $X+h$ where $h = -\frac{a}{3}$, we can get the cubic in the form
\[
	X^3 + PX + Q = 0
	.\]

where
\begin{align*}
	P & = -\frac{a^2}{3}+b                 \\
	Q & = \frac{2a^3}{27} - \frac{ab}{3}+c
\end{align*}

\subsection{Roots of $x^4+ax^3+bx^2+cx+d=0$}

If we replace $x$ with $X+h$ where $h = -\frac{a}{4}$, we can get the quartic in the form
\[
	X^4 + PX^2 + QX + R = 0
	.\]

where
\begin{align*}
	P & = -\frac{3a^2}{8}+b                   \\
	Q & = \frac{a^3}{8} -\frac{ab}{2} + c     \\
	R & = -\frac{3a^4}{256} -\frac{ac}{4} + d
\end{align*}

Let the circle $C$ be the circle of radius $\displaystyle\lt(-\frac{Q}{2},\frac{1-P}{2}\rt)$ and of radius $\displaystyle\sqrt{\lt(\frac{P-1}{2}\rt)^2 + \frac{Q^2}{4} - R}$.\\

The roots of the polynomial $X^4 + PX^2 + QX + R = 0$ are the intersection of the circle $C$ and the parabola $Y=X^2$


\chapter{Numerical Intergration}


Let $f$ be a continuous function on $[a,b]$ and $I=\int_a^b f(x)\,\dd{x}$

\section{Rectangle method}

We sample the domain of $f$ in to $n$ equal subintervals ($x_i - x_{i+1}=\frac{b-a}{n}=h$). The approximated value of $I$ becomes
\[
	\int_a^b f(x)\,\dd{x} \approx \frac{b-a}{n}\lt[ \sum_{i=0}^{n-1} f(x_i) \rt]
	.\]

such that $x_0=a$ and $x_1=b$\\

The error associated with this approximation is
\[
	|\varepsilon|\leq \frac{M_1}{2n}(b-a)^2
	.\]

where
\[
	M_1 = \sup_{[a,b]}\lt|f'(x)\rt|
	.\]

\section{Trapezoid method}

Same sampling as before.

\[
	\int_a^b f(x)\,\dd{x} \approx \frac{b-a}{2n}\lt(f(a)+f(b)+2\sum_{i=1}^{n-1}f(x_i)\rt)
	.\]
The error associated with this approximation is
\[
	|\varepsilon|\leq \frac{M_2}{12n^2}\lt|(b-a)^3\rt|
	.\]

where
\[
	M_2 = \sup_{[a,b]}\lt|f''(x)\rt|
	.\]

\section{Simpson's rule}

You get the point by now

\[
	\int_a^b f(x)\,\dd{x} \approx \frac{b-a}{6n}\lt(f(a)+f(b)+2\sum_{i=1}^{n-1}f(x_i) + 4\lt(
	f\lt(\frac{a+x_1}{2}\rt) +
	f\lt(\frac{x_1+x_2}{2}\rt) +
	\cdots
	f\lt(\frac{x_{n-1}+b}{2}\rt)
	\rt)
	\rt)
	.\]

The error associated with this approximation is
\[
	|\varepsilon|\leq \frac{\lt|(b-a)^5\rt|}{n^4}\frac{M_4}{2880}
	.\]

where
\[
	M_4 = \sup_{[a,b]}\lt|f^{(4)}(x)\rt|
	.\]


\section{Newton Cote's method}

Let $P$ be the Lagrange polynomial that interpolates the function $f$ at $(n+1)$ points $(x_0,f_0),(x_1,f_1),\dots,(x_n,f_n)$.
\[
	P(s) = \sum_{i=0}^{n}\ell_i(s)f(x_i)
	.\]

The approximated value of $I$ is
\[
	\int_a^b f(x)\,\dd{x} \approx (b-a)\sum_{i=0}^n H_i f_i
	.\]

where
\[
	H_i = \frac{1}{n}\int_0^n \ell_i(s)\,\dd{s}
	.\]

\chapter{Linear Systems}

Developed form of a linear system:

\begin{align*}
	a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n & = b_1 \\
	a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n & = b_2 \\
	\vdots                                     &       \\
	a_{n1}x_1 + a_{n2}x_2 + \cdots + a_{nn}x_n & = b_n
\end{align*}

Matrix form of a linear system:

\[
	A\va{x} = \va{b}
	.\]


\[
	A = \begin{bmatrix}
		a_{11} & a_{12} & \cdots & a_{1n} \\
		a_{21} & a_{22} & \cdots & a_{2n} \\
		\vdots &        &        &        \\
		a_{n1} & a_{n2} & \cdots & a_{nn}
	\end{bmatrix} \quad
	\va{x} = \begin{bmatrix}
		x_1    \\
		x_2    \\
		\vdots \\
		x_n
	\end{bmatrix}\quad
	\va{b} =\begin{bmatrix}
		b_1    \\
		b_2    \\
		\vdots \\
		b_n
	\end{bmatrix}
	.\]

\section{Direct Methods}

\subsection{Cramer's Rule}

\[
	x_i = \frac{\det A_i}{\det A}
	.\]

where $A_i$ is the matrix obtained by replacing the $i$-th column of $A$ by $\va{b}$. For example

\[
	A_1 = \begin{bmatrix}
		b_1    & a_{12} & \cdots & a_{1n} \\
		b_2    & a_{22} & \cdots & a_{2n} \\
		\vdots &        & \ddots &        \\
		b_n    & a_{n2} & \cdots & a_{nn}
	\end{bmatrix}
	.\]

\subsection{Gaussian Elimination}

This method converts the augmented matrix $[A,\va{b}]$ in to an upper triangular matrix.

\[
	[A,\va{b}] = \lt[
		\begin{array}{cccc|c}
			a_{11} & a_{12} & \cdots & a_{1n} & b_1    \\
			a_{21} & a_{22} & \cdots & a_{2n} & b_2    \\
			\vdots &        & \ddots & \vdots & \vdots \\
			a_{n1} & a_{n2} & \cdots & a_{nn} & b_n
		\end{array}
		\rt]
	.\]

The steps of this method are as follows:

\begin{enumerate}
	\ii Compose the augmented matrix $[A,\va{b}]$.
	\ii Find the max pivot element $a_{kk}$ by only switching rows. Where $k$ is the current iteration. Then divide the row with the max pivot element by $a_{kk}$.
	\ii Subtract all the rows below the current row by a multiple of the current row such that the elements below the pivot element are zero.
	\ii Repeat the above steps until you reach the last row.
	\ii Find the value of $x_n$ and substitute it in the $n-1$-th equation until you have reached the first equation.
\end{enumerate}

\subsection{Gauss-Jordan Elimination}

This method is identical to Gaussian elimination except that it converts the augmented matrix $[A,\va{b}]$ in to an augmented identity matrix. Simply perform the steps of Gaussian elimination and then subtract the $n-1$-th equation with the $n$-th equation times a constant so that the non-pivot terms becomes zero. The final step is repeated until the matrix is diagonal.


\section{Iterative Methods}

\subsection{Jacobi's Method}

The steps of this method are as follows:

\begin{enumerate}
	\ii Arrange the given system of equations in a diagonally dominant form.
	\ii Assume an initial guess for the solution vector $\va{x}^{(0)}$.
	\ii Calculate the next iteration of the solution vector $\va{x}^{(k+1)}$ using the following formula
	\[
		x_i^{(k+1)} = \frac{1}{a_{ii}}\lt(b_i - \sum_{\substack{j=1\\j\neq i}}^{n}a_{ij}x_j^{(k)}\rt)
		.\]
\end{enumerate}

\nt{
A matrix $A$ is said to be diagonally dominant if
\[
	|a_{ii}| > \sum_{\substack{j=1\\j\neq i}}^{n}|a_{ij}|
	.\]
}

\subsection{Gauss-Seidel Method}

This method is identical to Jacobi's method except that the solution vector $\va{x}^{(k+1)}$ is calculated using the following formula

\[
	x_i^{(k+1)} = \frac{1}{a_{ii}}\lt(b_i - \sum_{j=1}^{i-1}a_{ij}x_j^{(k+1)} - \sum_{j=i+1}^{n}a_{ij}x_j^{(k)}\rt)
	.\]
which uses the most recent values of the solution vector instead of the previous iteration.

\subsubsection{Matrix Form}

The matrix $A$ can be written as the sum of a lower triangular matrix $E$, a diagonal matrix $D$ and an upper triangular matrix $F$.

\[
	A = D - E - F
	.\]
The Gauss-Seidel method can be written in matrix form as follows

\[
	\va{x}^{(k+1)} = (D-E)^{-1}F\cdot\va{x}^{(k)} + (D-E)^{-1}\cdot\va{b}
	.\]

\nt{
	If the matrix $A$ is diagonally dominant then the Gauss-Seidel method and the Jacobi method converge.
}

\nt{
The stop condition for iterative methods is
\[
	\lt|x_i^{(k+1)} - x_i^{(k)}\rt| < \varepsilon\quad\forall i
	.\]
}

\subsubsection{Successive Over-Relaxation (SOR)}

The choice of initial guess for the solution vector $\va{x}^{(0)}$ can affect the convergence of the iterative methods. The SOR multiplies the difference between the current and the next iteration of the solution vector by a constant $\lambda$.

\subsubsection{Same Side Convergence}

If $(x^{(1)} - x^{(2)})(x^{(2)} - x^{(3)}) > 0$ then we accelerate the convergence by multiplying the difference by a constant $\lambda > 1$ (usually $\lambda=1.2$). Then we calculate

\[
	x^{(2)*} = x^{(1)} + \lambda(x^{(2)} - x^{(1)})
	.\]

\subsubsection{Opposite Side Convergence}

If $(x^{(1)} - x^{(2)})(x^{(2)} - x^{(3)}) < 0$ then we accelerate the convergence by multiplying the difference by a constant $\lambda < 1$ (usually $\lambda=0.8$). Then we calculate

\[
	x^{(2)*} = x^{(1)} + \lambda(x^{(2)} - x^{(1)})
	.\]

\chapter{Differential Equations}

\dfn{Cauchy's Problem}{
	Cauchy's problem is to find a function $y(x)$ given a function $f$ such that
	\[
		\begin{cases}
			\dv{y}{x} = f(x,y) \\
			y(x_0) = y_0
		\end{cases}
	\]
}

\thm{}{
	There exists a unique solution to Cauchy's problem if $f$ is continuous on $[a,b]$ and Lipschitz continuous in $y$.
	\[
		\exists L \in \RR^+ \Big/ \lt|f(x,y_1) - f(x,y_2)\rt| \leq L\lt|y_1 - y_2\rt|\quad\forall x\in[a,b],\forall y_1,y_2\in\RR
		.\]
}

\mlenma{}{
	If $f_y$ is continuous and bounded then $f$ is Lipschitz continuous in $y$.
}

\section{Convergence and Stability}

Choosing a method means choosing a function $\varphi$ such that
\[
	y_{i+1} = y_i +h\cdot\varphi(x_i,y_i,h)
	.\]

\dfn{Convergence}{
	A method is said to be convergent if
	\[
		\lim_{h\to 0}\max_{i=0,\ldots,n-1}\lt|y(x_i) - y_i\rt| = 0
		.\]
}

\dfn{Consistency}{
A method is said to be consistent if
\[
\lim_{h\to 0}\max_{i=0,\ldots,n-1}\frac{1}{h}(y(x_{i+1}) - y(x_i) - h\cdot\varphi(x_i,y(x_i),h)) = 0
.\]
or in other words
\[
\varphi(x,y,0) = f(x,y)
.\]
}

\dfn{Stability}{
Let $y_i$ and $z_i$ be the solutions of the following problems
\[
	\begin{cases}
		y_{i+1} = y_i + h\cdot\varphi(x_i,y_i,h) \\
		y(x_0) = y_0
	\end{cases}
	\quad\text{and}\quad
	\begin{cases}
		z_{i+1} = z_i + h\cdot[\varphi(x_i,z_i,h)+\varepsilon_i] \\
		z(x_0) = z_0
	\end{cases}
\]

where $\lim_{h\to 0}\max_i |\varepsilon_i| = 0$. The method is said to be stable if
\[
	\exists M_1,M_2,\forall h \Big/ \max_i |y_{i+1} - z_{i+1}| \leq  M_1\max_i |y_0 - z_0| + M_2\max_i |\varepsilon_i|
	.\]
}

\thm{}{
	If $\varphi$ is Lipschitz continuous in $y$ then the method is stable.
}

\thm{}{
	If the method is stable and consistent then it is convergent.
}

\thm{}{
	If $\varphi(x,y,0) = f(x,y)$ and $\varphi$ is Lipschitz continuous in $y$ then the method is stable.
}

\section{Euler's Method}

For all methods here on out we will divide the interval $[a,b]$ into $n$ sub-intervals of length $h$ such that $h = \frac{b-a}{n}$ (equidistant).

\[
	y_{i+1} = y_i + h\cdot f(x_i,y_i)
	.\]

where $y_i$ is the approximation of $y(x_i)$.

\section{Heun's Method}

\[
	y_{i+1} = y_i + \frac{h}{2}\lt(f(x_i,y_i) + f(x_{i+1},y_i + h\cdot f(x_i,y_i))\rt)
	.\]


\section{Taylor's Method}

\[
	y_{i+1} = y_i + h\cdot f(x_i,y_i) + \frac{h^2}{2!}\cdot y'' + \frac{h^3}{3!}\cdot y''' + \ldots
	.\]

\section{Runge-Kutta Methods}

\subsection{RK2}

\[
	y_{i+1} = y_i + \frac{h}{2}\cdot\lt(k_1 + k_2\rt)
	.\]

where
\begin{align*}
	k_1 & = f(x_i,y_i)                  \\
	k_2 & = f(x_i + h,y_i + h\cdot k_1)
\end{align*}

\subsection{RK4}

\[
	y_{i+1} = y_i + \frac{h}{6}\cdot\lt(k_1 + 2k_2 + 2k_3 + k_4\rt)
	.\]

where
\begin{align*}
	k_1 & = f(x_i,y_i)                                            \\
	k_2 & = f\lt(x_i + \frac{h}{2},y_i + \frac{h}{2}\cdot k_1\rt) \\
	k_3 & = f\lt(x_i + \frac{h}{2},y_i + \frac{h}{2}\cdot k_2\rt) \\
	k_4 & = f(x_i + h,y_i + h\cdot k_3)
\end{align*}

\end{document}
